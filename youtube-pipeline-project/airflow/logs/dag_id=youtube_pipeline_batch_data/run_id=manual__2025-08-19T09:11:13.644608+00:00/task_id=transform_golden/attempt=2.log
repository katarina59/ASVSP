[2025-08-19T09:20:42.047+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-08-19T09:20:42.129+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-19T09:11:13.644608+00:00 [queued]>
[2025-08-19T09:20:42.149+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-19T09:11:13.644608+00:00 [queued]>
[2025-08-19T09:20:42.150+0000] {taskinstance.py:2303} INFO - Starting attempt 2 of 4
[2025-08-19T09:20:42.182+0000] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): transform_golden> on 2025-08-19 09:11:13.644608+00:00
[2025-08-19T09:20:42.190+0000] {standard_task_runner.py:63} INFO - Started process 2287 to run task
[2025-08-19T09:20:42.196+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'youtube_pipeline_batch_data', 'transform_golden', 'manual__2025-08-19T09:11:13.644608+00:00', '--job-id', '600', '--raw', '--subdir', 'DAGS_FOLDER/youtube_pipeline.py', '--cfg-path', '/tmp/tmpevwne2hd']
[2025-08-19T09:20:42.200+0000] {standard_task_runner.py:91} INFO - Job 600: Subtask transform_golden
[2025-08-19T09:20:42.325+0000] {task_command.py:426} INFO - Running <TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-19T09:11:13.644608+00:00 [running]> on host 54bf3a3eaeab
[2025-08-19T09:20:42.641+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='youtube_pipeline_batch_data' AIRFLOW_CTX_TASK_ID='transform_golden' AIRFLOW_CTX_EXECUTION_DATE='2025-08-19T09:11:13.644608+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-19T09:11:13.644608+00:00'
[2025-08-19T09:20:42.643+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-08-19T09:20:42.675+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-08-19T09:20:42.676+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'docker exec spark-master spark-submit --master spark://spark-master:7077 --deploy-mode client --packages org.apache.spark:spark-avro_2.12:3.5.1 /opt/***/jobs/transformation/transform_to_golden_dataset.py']
[2025-08-19T09:20:42.694+0000] {subprocess.py:86} INFO - Output:
[2025-08-19T09:20:51.803+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-08-19T09:20:52.112+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2025-08-19T09:20:52.115+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-08-19T09:20:52.127+0000] {subprocess.py:93} INFO - org.apache.spark#spark-avro_2.12 added as a dependency
[2025-08-19T09:20:52.128+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-ebb88d74-490a-4af4-94b3-46dec86546b7;1.0
[2025-08-19T09:20:52.128+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-08-19T09:20:53.848+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-avro_2.12;3.5.1 in central
[2025-08-19T09:20:54.433+0000] {subprocess.py:93} INFO - 	found org.tukaani#xz;1.9 in central
[2025-08-19T09:20:54.817+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 2609ms :: artifacts dl 82ms
[2025-08-19T09:20:54.821+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-08-19T09:20:54.822+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-avro_2.12;3.5.1 from central in [default]
[2025-08-19T09:20:54.825+0000] {subprocess.py:93} INFO - 	org.tukaani#xz;1.9 from central in [default]
[2025-08-19T09:20:54.826+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-19T09:20:54.829+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-08-19T09:20:54.829+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-08-19T09:20:54.834+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-19T09:20:54.834+0000] {subprocess.py:93} INFO - 	|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
[2025-08-19T09:20:54.835+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-19T09:20:54.855+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-ebb88d74-490a-4af4-94b3-46dec86546b7
[2025-08-19T09:20:54.862+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-08-19T09:20:54.953+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 2 already retrieved (0kB/90ms)
[2025-08-19T09:20:56.810+0000] {subprocess.py:93} INFO - 25/08/19 09:20:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-19T09:21:00.853+0000] {subprocess.py:93} INFO - 25/08/19 09:21:00 INFO SparkContext: Running Spark version 3.5.1
[2025-08-19T09:21:00.856+0000] {subprocess.py:93} INFO - 25/08/19 09:21:00 INFO SparkContext: OS info Linux, 6.11.0-29-generic, amd64
[2025-08-19T09:21:00.866+0000] {subprocess.py:93} INFO - 25/08/19 09:21:00 INFO SparkContext: Java version 17.0.12
[2025-08-19T09:21:01.093+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO ResourceUtils: ==============================================================
[2025-08-19T09:21:01.095+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-19T09:21:01.097+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO ResourceUtils: ==============================================================
[2025-08-19T09:21:01.105+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO SparkContext: Submitted application: Processed Zone - Golden Dataset
[2025-08-19T09:21:01.181+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-19T09:21:01.198+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO ResourceProfile: Limiting resource is cpu
[2025-08-19T09:21:01.204+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-19T09:21:01.345+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO SecurityManager: Changing view acls to: spark
[2025-08-19T09:21:01.349+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO SecurityManager: Changing modify acls to: spark
[2025-08-19T09:21:01.350+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO SecurityManager: Changing view acls groups to:
[2025-08-19T09:21:01.355+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO SecurityManager: Changing modify acls groups to:
[2025-08-19T09:21:01.355+0000] {subprocess.py:93} INFO - 25/08/19 09:21:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-08-19T09:21:03.985+0000] {subprocess.py:93} INFO - 25/08/19 09:21:03 INFO Utils: Successfully started service 'sparkDriver' on port 40623.
[2025-08-19T09:21:04.161+0000] {subprocess.py:93} INFO - 25/08/19 09:21:04 INFO SparkEnv: Registering MapOutputTracker
[2025-08-19T09:21:04.425+0000] {subprocess.py:93} INFO - 25/08/19 09:21:04 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-19T09:21:04.619+0000] {subprocess.py:93} INFO - 25/08/19 09:21:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-19T09:21:04.629+0000] {subprocess.py:93} INFO - 25/08/19 09:21:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-19T09:21:04.656+0000] {subprocess.py:93} INFO - 25/08/19 09:21:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-19T09:21:04.883+0000] {subprocess.py:93} INFO - 25/08/19 09:21:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-53a46c5b-2fab-4b44-99f4-9efd3eb6febf
[2025-08-19T09:21:05.054+0000] {subprocess.py:93} INFO - 25/08/19 09:21:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-19T09:21:05.127+0000] {subprocess.py:93} INFO - 25/08/19 09:21:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-19T09:21:05.911+0000] {subprocess.py:93} INFO - 25/08/19 09:21:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-19T09:21:06.435+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-08-19T09:21:06.532+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-08-19T09:21:06.757+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar at spark://b208641feb52:40623/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar with timestamp 1755595260820
[2025-08-19T09:21:06.759+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://b208641feb52:40623/jars/org.tukaani_xz-1.9.jar with timestamp 1755595260820
[2025-08-19T09:21:06.768+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar at spark://b208641feb52:40623/files/org.apache.spark_spark-avro_2.12-3.5.1.jar with timestamp 1755595260820
[2025-08-19T09:21:06.775+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar to /tmp/spark-4d32c31f-c646-4a62-98b0-5ccd58ca2c08/userFiles-3111ba4b-ace0-4a61-a47e-cb6930b5874c/org.apache.spark_spark-avro_2.12-3.5.1.jar
[2025-08-19T09:21:06.830+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://b208641feb52:40623/files/org.tukaani_xz-1.9.jar with timestamp 1755595260820
[2025-08-19T09:21:06.833+0000] {subprocess.py:93} INFO - 25/08/19 09:21:06 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar to /tmp/spark-4d32c31f-c646-4a62-98b0-5ccd58ca2c08/userFiles-3111ba4b-ace0-4a61-a47e-cb6930b5874c/org.tukaani_xz-1.9.jar
[2025-08-19T09:21:07.654+0000] {subprocess.py:93} INFO - 25/08/19 09:21:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-19T09:21:07.928+0000] {subprocess.py:93} INFO - 25/08/19 09:21:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 156 ms (0 ms spent in bootstraps)
[2025-08-19T09:21:08.135+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250819092108-0008
[2025-08-19T09:21:08.172+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43703.
[2025-08-19T09:21:08.174+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO NettyBlockTransferService: Server created on b208641feb52:43703
[2025-08-19T09:21:08.181+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-19T09:21:08.197+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b208641feb52, 43703, None)
[2025-08-19T09:21:08.241+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO BlockManagerMasterEndpoint: Registering block manager b208641feb52:43703 with 434.4 MiB RAM, BlockManagerId(driver, b208641feb52, 43703, None)
[2025-08-19T09:21:08.282+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b208641feb52, 43703, None)
[2025-08-19T09:21:08.302+0000] {subprocess.py:93} INFO - 25/08/19 09:21:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b208641feb52, 43703, None)
[2025-08-19T09:21:09.872+0000] {subprocess.py:93} INFO - 25/08/19 09:21:09 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-19T09:21:11.256+0000] {subprocess.py:93} INFO - 25/08/19 09:21:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-19T09:21:11.260+0000] {subprocess.py:93} INFO - 25/08/19 09:21:11 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2025-08-19T09:21:15.685+0000] {subprocess.py:93} INFO - 25/08/19 09:21:15 INFO InMemoryFileIndex: It took 261 ms to list leaf files for 1 paths.
[2025-08-19T09:21:19.617+0000] {subprocess.py:93} INFO - 25/08/19 09:21:19 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
[2025-08-19T09:21:24.330+0000] {subprocess.py:93} INFO - 25/08/19 09:21:24 INFO InMemoryFileIndex: It took 85 ms to list leaf files for 1 paths.
[2025-08-19T09:21:24.814+0000] {subprocess.py:93} INFO - 25/08/19 09:21:24 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
[2025-08-19T09:21:28.210+0000] {subprocess.py:93} INFO - 25/08/19 09:21:28 INFO InMemoryFileIndex: It took 70 ms to list leaf files for 1 paths.
[2025-08-19T09:21:28.473+0000] {subprocess.py:93} INFO - 25/08/19 09:21:28 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
[2025-08-19T09:21:30.203+0000] {subprocess.py:93} INFO - 25/08/19 09:21:30 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.
[2025-08-19T09:21:30.399+0000] {subprocess.py:93} INFO - 25/08/19 09:21:30 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.
[2025-08-19T09:21:33.127+0000] {subprocess.py:93} INFO - 25/08/19 09:21:33 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
[2025-08-19T09:21:33.368+0000] {subprocess.py:93} INFO - 25/08/19 09:21:33 INFO InMemoryFileIndex: It took 55 ms to list leaf files for 1 paths.
[2025-08-19T09:21:37.091+0000] {subprocess.py:93} INFO - 25/08/19 09:21:37 INFO InMemoryFileIndex: It took 74 ms to list leaf files for 1 paths.
[2025-08-19T09:21:37.749+0000] {subprocess.py:93} INFO - 25/08/19 09:21:37 INFO InMemoryFileIndex: It took 161 ms to list leaf files for 1 paths.
[2025-08-19T09:21:42.808+0000] {subprocess.py:93} INFO - 25/08/19 09:21:42 INFO InMemoryFileIndex: It took 154 ms to list leaf files for 1 paths.
[2025-08-19T09:21:43.326+0000] {subprocess.py:93} INFO - 25/08/19 09:21:43 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
[2025-08-19T09:21:45.916+0000] {subprocess.py:93} INFO - 25/08/19 09:21:45 INFO InMemoryFileIndex: It took 99 ms to list leaf files for 1 paths.
[2025-08-19T09:21:46.523+0000] {subprocess.py:93} INFO - 25/08/19 09:21:46 INFO InMemoryFileIndex: It took 111 ms to list leaf files for 1 paths.
[2025-08-19T09:21:50.114+0000] {subprocess.py:93} INFO - 25/08/19 09:21:50 INFO InMemoryFileIndex: It took 103 ms to list leaf files for 1 paths.
[2025-08-19T09:21:50.513+0000] {subprocess.py:93} INFO - 25/08/19 09:21:50 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.
[2025-08-19T09:21:52.810+0000] {subprocess.py:93} INFO - 25/08/19 09:21:52 INFO InMemoryFileIndex: It took 88 ms to list leaf files for 1 paths.
[2025-08-19T09:21:53.477+0000] {subprocess.py:93} INFO - 25/08/19 09:21:53 INFO InMemoryFileIndex: It took 123 ms to list leaf files for 1 paths.
[2025-08-19T09:22:05.856+0000] {subprocess.py:93} INFO - 25/08/19 09:22:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:05.892+0000] {subprocess.py:93} INFO - 25/08/19 09:22:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#7),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None) END),(cast(views#7 as bigint) > 0)
[2025-08-19T09:22:05.939+0000] {subprocess.py:93} INFO - 25/08/19 09:22:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:05.945+0000] {subprocess.py:93} INFO - 25/08/19 09:22:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#32)
[2025-08-19T09:22:06.006+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.012+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#500),isnotnull(CASE WHEN (length(trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None) END),(cast(views#500 as bigint) > 0)
[2025-08-19T09:22:06.018+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.020+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#525)
[2025-08-19T09:22:06.165+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.178+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#993),isnotnull(CASE WHEN (length(trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None) END),(cast(views#993 as bigint) > 0)
[2025-08-19T09:22:06.188+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.192+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#1018)
[2025-08-19T09:22:06.270+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.276+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#1486),isnotnull(CASE WHEN (length(trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None) END),(cast(views#1486 as bigint) > 0)
[2025-08-19T09:22:06.281+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.284+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#1511)
[2025-08-19T09:22:06.408+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.443+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#1979),isnotnull(CASE WHEN (length(trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None) END),(cast(views#1979 as bigint) > 0)
[2025-08-19T09:22:06.444+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.444+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2004)
[2025-08-19T09:22:06.472+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.473+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#2472),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None) END),(cast(views#2472 as bigint) > 0)
[2025-08-19T09:22:06.474+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.476+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2497)
[2025-08-19T09:22:06.555+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.594+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#2965),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None) END),(cast(views#2965 as bigint) > 0)
[2025-08-19T09:22:06.635+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.637+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2990)
[2025-08-19T09:22:06.922+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:06.923+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#3458),isnotnull(CASE WHEN (length(trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None) END),(cast(views#3458 as bigint) > 0)
[2025-08-19T09:22:06.953+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:06.961+0000] {subprocess.py:93} INFO - 25/08/19 09:22:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#3483)
[2025-08-19T09:22:07.063+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:07.073+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#3951),isnotnull(CASE WHEN (length(trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None) END),(cast(views#3951 as bigint) > 0)
[2025-08-19T09:22:07.076+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:07.079+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#3976)
[2025-08-19T09:22:07.327+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-19T09:22:07.341+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#4444),isnotnull(CASE WHEN (length(trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None) END),(cast(views#4444 as bigint) > 0)
[2025-08-19T09:22:07.358+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-19T09:22:07.371+0000] {subprocess.py:93} INFO - 25/08/19 09:22:07 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#4469)
[2025-08-19T09:22:08.494+0000] {subprocess.py:93} INFO - 25/08/19 09:22:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-08-19T09:22:16.999+0000] {subprocess.py:93} INFO - 25/08/19 09:22:16 INFO CodeGenerator: Code generated in 2594.030019 ms
[2025-08-19T09:22:17.002+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 2553.510694 ms
[2025-08-19T09:22:17.061+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 2603.438114 ms
[2025-08-19T09:22:17.149+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 2558.603099 ms
[2025-08-19T09:22:17.225+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 2751.269521 ms
[2025-08-19T09:22:17.296+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 2748.574822 ms
[2025-08-19T09:22:17.373+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 2906.249285 ms
[2025-08-19T09:22:17.418+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 3047.539668 ms
[2025-08-19T09:22:17.699+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 3055.664416 ms
[2025-08-19T09:22:17.729+0000] {subprocess.py:93} INFO - 25/08/19 09:22:17 INFO CodeGenerator: Code generated in 3053.626456 ms
[2025-08-19T09:22:18.013+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.9 KiB, free 434.2 MiB)
[2025-08-19T09:22:18.023+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.9 KiB, free 434.0 MiB)
[2025-08-19T09:22:18.048+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.9 KiB, free 433.8 MiB)
[2025-08-19T09:22:18.075+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 200.9 KiB, free 433.4 MiB)
[2025-08-19T09:22:18.081+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.9 KiB, free 433.2 MiB)
[2025-08-19T09:22:18.106+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.9 KiB, free 433.6 MiB)
[2025-08-19T09:22:18.110+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.9 KiB, free 433.0 MiB)
[2025-08-19T09:22:18.118+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.9 KiB, free 432.8 MiB)
[2025-08-19T09:22:18.141+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.9 KiB, free 432.6 MiB)
[2025-08-19T09:22:18.160+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.9 KiB, free 432.4 MiB)
[2025-08-19T09:22:18.701+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.4 MiB)
[2025-08-19T09:22:18.705+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-19T09:22:18.719+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO SparkContext: Created broadcast 1 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:18.730+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:18.983+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.4 MiB)
[2025-08-19T09:22:18.990+0000] {subprocess.py:93} INFO - 25/08/19 09:22:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-19T09:22:19.006+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-19T09:22:19.016+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:22:19.033+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-19T09:22:19.085+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:22:19.096+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:22:19.166+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:22:19.174+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.175+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-19T09:22:19.183+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:22:19.189+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.207+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.208+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167907 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.209+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-19T09:22:19.210+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-19T09:22:19.210+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.211+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.226+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-19T09:22:19.240+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.246+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.251+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-19T09:22:19.256+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:22:19.264+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-19T09:22:19.265+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.266+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.269+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.270+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.289+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-19T09:22:19.291+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b208641feb52:43703 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-19T09:22:19.294+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.298+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.300+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.313+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:19.326+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:19.362+0000] {subprocess.py:93} INFO - 25/08/19 09:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-19T09:22:21.183+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.230+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.327+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.343+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.408+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.427+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.447+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.460+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.476+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.484+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-19T09:22:21.504+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO DAGScheduler: Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:21.517+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:21.520+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:21.557+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:21.609+0000] {subprocess.py:93} INFO - 25/08/19 09:22:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:22.144+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.1 KiB, free 432.1 MiB)
[2025-08-19T09:22:22.155+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.1 MiB)
[2025-08-19T09:22:22.160+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-19T09:22:22.174+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:22.468+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:22.613+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2025-08-19T09:22:22.984+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:22.995+0000] {subprocess.py:93} INFO - 25/08/19 09:22:22 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.006+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.011+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.027+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.067+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.1 KiB, free 432.1 MiB)
[2025-08-19T09:22:23.103+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.1 MiB)
[2025-08-19T09:22:23.114+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.115+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.131+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.132+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.188+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.190+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.197+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.207+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.216+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.414+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.426+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.448+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.455+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.456+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.469+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.470+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.477+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.477+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.478+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.500+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.500+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.503+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.515+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.516+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.516+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.516+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.517+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.517+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.517+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.518+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.521+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.522+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.525+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.525+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.531+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.532+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.535+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.537+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.538+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.541+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.543+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.544+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.544+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.544+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.545+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.553+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.554+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.554+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.555+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.555+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.564+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.565+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.565+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.565+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.569+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-19T09:22:23.575+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.575+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.576+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.579+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.582+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.583+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.584+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.589+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.590+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.595+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-19T09:22:23.609+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-19T09:22:23.613+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.613+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.615+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.616+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.626+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.626+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.627+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.627+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.628+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.699+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-19T09:22:23.717+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-19T09:22:23.726+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.755+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.766+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.767+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0
[2025-08-19T09:22:23.786+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-19T09:22:23.788+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-19T09:22:23.788+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-19T09:22:23.796+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Missing parents: List()
[2025-08-19T09:22:23.815+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-19T09:22:23.860+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-19T09:22:23.891+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-19T09:22:23.915+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on b208641feb52:43703 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-19T09:22:23.932+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-08-19T09:22:23.941+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-19T09:22:23.949+0000] {subprocess.py:93} INFO - 25/08/19 09:22:23 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0
[2025-08-19T09:22:37.969+0000] {subprocess.py:93} INFO - 25/08/19 09:22:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:22:52.966+0000] {subprocess.py:93} INFO - 25/08/19 09:22:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:23:07.975+0000] {subprocess.py:93} INFO - 25/08/19 09:23:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:23:22.965+0000] {subprocess.py:93} INFO - 25/08/19 09:23:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:23:37.965+0000] {subprocess.py:93} INFO - 25/08/19 09:23:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:23:52.969+0000] {subprocess.py:93} INFO - 25/08/19 09:23:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:24:07.965+0000] {subprocess.py:93} INFO - 25/08/19 09:24:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:24:22.966+0000] {subprocess.py:93} INFO - 25/08/19 09:24:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:24:37.971+0000] {subprocess.py:93} INFO - 25/08/19 09:24:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:24:52.969+0000] {subprocess.py:93} INFO - 25/08/19 09:24:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:25:07.965+0000] {subprocess.py:93} INFO - 25/08/19 09:25:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:25:22.971+0000] {subprocess.py:93} INFO - 25/08/19 09:25:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:25:37.965+0000] {subprocess.py:93} INFO - 25/08/19 09:25:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:25:52.968+0000] {subprocess.py:93} INFO - 25/08/19 09:25:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:26:07.972+0000] {subprocess.py:93} INFO - 25/08/19 09:26:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:26:22.972+0000] {subprocess.py:93} INFO - 25/08/19 09:26:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:26:37.989+0000] {subprocess.py:93} INFO - 25/08/19 09:26:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:26:52.969+0000] {subprocess.py:93} INFO - 25/08/19 09:26:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:27:07.968+0000] {subprocess.py:93} INFO - 25/08/19 09:27:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:27:22.966+0000] {subprocess.py:93} INFO - 25/08/19 09:27:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:27:37.982+0000] {subprocess.py:93} INFO - 25/08/19 09:27:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:27:52.964+0000] {subprocess.py:93} INFO - 25/08/19 09:27:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:28:07.979+0000] {subprocess.py:93} INFO - 25/08/19 09:28:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:28:22.965+0000] {subprocess.py:93} INFO - 25/08/19 09:28:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:28:34.004+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250819092108-0008/0 on worker-20250819090904-172.18.0.12-34859 (172.18.0.12:34859) with 4 core(s)
[2025-08-19T09:28:34.035+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250819092108-0008/0 on hostPort 172.18.0.12:34859 with 4 core(s), 1024.0 MiB RAM
[2025-08-19T09:28:34.042+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250819092108-0008/1 on worker-20250819090904-172.18.0.17-46791 (172.18.0.17:46791) with 4 core(s)
[2025-08-19T09:28:34.055+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250819092108-0008/1 on hostPort 172.18.0.17:46791 with 4 core(s), 1024.0 MiB RAM
[2025-08-19T09:28:34.055+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250819092108-0008/2 on worker-20250819090906-172.18.0.16-36117 (172.18.0.16:36117) with 4 core(s)
[2025-08-19T09:28:34.067+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250819092108-0008/2 on hostPort 172.18.0.16:36117 with 4 core(s), 1024.0 MiB RAM
[2025-08-19T09:28:34.461+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250819092108-0008/2 is now RUNNING
[2025-08-19T09:28:34.497+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250819092108-0008/1 is now RUNNING
[2025-08-19T09:28:34.520+0000] {subprocess.py:93} INFO - 25/08/19 09:28:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250819092108-0008/0 is now RUNNING
[2025-08-19T09:28:37.982+0000] {subprocess.py:93} INFO - 25/08/19 09:28:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:28:52.965+0000] {subprocess.py:93} INFO - 25/08/19 09:28:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-19T09:28:53.209+0000] {subprocess.py:93} INFO - 25/08/19 09:28:53 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:51542) with ID 0,  ResourceProfileId 0
[2025-08-19T09:28:53.712+0000] {subprocess.py:93} INFO - 25/08/19 09:28:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:33395 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.12, 33395, None)
[2025-08-19T09:28:54.088+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.16:39858) with ID 2,  ResourceProfileId 0
[2025-08-19T09:28:54.435+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.17:45858) with ID 1,  ResourceProfileId 0
[2025-08-19T09:28:54.514+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.16:34637 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.16, 34637, None)
[2025-08-19T09:28:54.895+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.17:39563 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.17, 39563, None)
[2025-08-19T09:28:54.998+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:28:54.999+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:28:55.005+0000] {subprocess.py:93} INFO - 25/08/19 09:28:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:28:55.011+0000] {subprocess.py:93} INFO - 25/08/19 09:28:55 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:28:55.523+0000] {subprocess.py:93} INFO - 25/08/19 09:28:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:28:55.529+0000] {subprocess.py:93} INFO - 25/08/19 09:28:55 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:28:55.538+0000] {subprocess.py:93} INFO - 25/08/19 09:28:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:28:55.542+0000] {subprocess.py:93} INFO - 25/08/19 09:28:55 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:28:56.108+0000] {subprocess.py:93} INFO - 25/08/19 09:28:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8) (172.18.0.17, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-19T09:28:56.108+0000] {subprocess.py:93} INFO - 25/08/19 09:28:56 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9) (172.18.0.17, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-19T09:28:56.134+0000] {subprocess.py:93} INFO - 25/08/19 09:28:56 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10) (172.18.0.17, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-19T09:28:56.134+0000] {subprocess.py:93} INFO - 25/08/19 09:28:56 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11) (172.18.0.17, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-19T09:28:57.082+0000] {subprocess.py:93} INFO - 25/08/19 09:28:57 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.12:33395 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-19T09:28:57.097+0000] {subprocess.py:93} INFO - 25/08/19 09:28:57 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.12:33395 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-19T09:28:57.735+0000] {subprocess.py:93} INFO - 25/08/19 09:28:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.16:34637 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-19T09:28:57.741+0000] {subprocess.py:93} INFO - 25/08/19 09:28:57 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.16:34637 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-19T09:28:58.632+0000] {subprocess.py:93} INFO - 25/08/19 09:28:58 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.17:39563 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-19T09:28:58.633+0000] {subprocess.py:93} INFO - 25/08/19 09:28:58 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.17:39563 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-19T09:29:01.862+0000] {subprocess.py:93} INFO - 25/08/19 09:29:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.16:34637 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-19T09:29:01.865+0000] {subprocess.py:93} INFO - 25/08/19 09:29:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.16:34637 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:04.208+0000] {subprocess.py:93} INFO - 25/08/19 09:29:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.12:33395 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-19T09:29:04.232+0000] {subprocess.py:93} INFO - 25/08/19 09:29:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.12:33395 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:07.644+0000] {subprocess.py:93} INFO - 25/08/19 09:29:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.17:39563 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-19T09:29:07.662+0000] {subprocess.py:93} INFO - 25/08/19 09:29:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.17:39563 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:11.038+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:11.095+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:11.160+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:11.214+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 15) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:11.472+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 6) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:11.476+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:11.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:11.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:11.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:11.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:11.498+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:11.498+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-08-19T09:29:11.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:11.501+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:11.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:11.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:11.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:11.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:11.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:11.529+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:11.530+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:11.534+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:11.541+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:11.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:11.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:11.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:11.549+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:11.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:11.560+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:11.560+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:11.564+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:11.565+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:11.566+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.16:34637 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:11.567+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 7) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:11.573+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:11.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:11.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:11.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:11.580+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:11.580+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:11.580+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-08-19T09:29:11.581+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:11.583+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:11.584+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:11.589+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:11.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:11.591+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:11.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:11.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:11.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:11.596+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:11.597+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:11.600+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:11.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:11.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:11.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:11.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:11.616+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:11.617+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:11.619+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:11.621+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:11.624+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.16:34637 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:11.626+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 5) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:11.628+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:11.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:11.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:11.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:11.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:11.631+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:11.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-19T09:29:11.635+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:11.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:11.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:11.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:11.642+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:11.647+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:11.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:11.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:11.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:11.651+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:11.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:11.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:11.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:11.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:11.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:11.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:11.665+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:11.667+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:11.669+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:11.671+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:11.673+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:11.673+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:11.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:11.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:11.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:11.695+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:11.697+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:11.699+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-19T09:29:11.701+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:11.701+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:11.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:11.710+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:11.712+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:11.713+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:11.714+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:11.716+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:11.718+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:11.721+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:11.722+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:11.722+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:11.723+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:11.723+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:11.723+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:11.724+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:11.724+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:11.724+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:11.725+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:11.726+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:11.962+0000] {subprocess.py:93} INFO - 25/08/19 09:29:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.16:34637 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:12.179+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.16:34637 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:12.523+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 16) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:12.586+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 17) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:12.612+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 1.1 in stage 3.0 (TID 18) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:12.619+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 19) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:12.628+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:12.643+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:12.644+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 14) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00005-05b9d236-db8d-4e1a-a709-8449bc2fd6ef-c000.avro
[2025-08-19T09:29:12.644+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.645+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.645+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.646+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.678+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.678+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-08-19T09:29:12.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.703+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.704+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:12.724+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:12.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:12.742+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:12.759+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:12.761+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:12.761+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:12.762+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:12.762+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:12.762+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:12.763+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:12.775+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:12.775+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:12.776+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:12.798+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.818+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.819+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-19T09:29:12.819+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.825+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.834+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.835+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.836+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.844+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:12.850+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:12.851+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:12.853+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:12.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:12.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:12.866+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:12.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:12.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:12.877+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:12.878+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:12.882+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:12.882+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:12.887+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 3) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:12.891+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.898+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.921+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-19T09:29:12.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:12.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:12.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:12.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:12.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:12.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:12.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:12.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:12.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:12.959+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:12.960+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:12.960+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:12.960+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:12.961+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00005-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:12.961+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.962+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.962+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.963+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.963+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.963+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.964+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
[2025-08-19T09:29:12.964+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.964+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.965+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.967+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.967+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.968+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.968+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.968+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:12.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:12.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:12.971+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:12.971+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:12.971+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:12.972+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:12.972+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:12.972+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.974+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
[2025-08-19T09:29:12.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.978+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.978+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.978+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:12.979+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:12.979+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:12.979+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:12.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:12.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:12.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:12.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:12.981+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:12.981+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:12.981+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:12.982+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:12.982+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:12.982+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 22) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:12.983+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 23) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:12.983+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 15) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-05b9d236-db8d-4e1a-a709-8449bc2fd6ef-c000.avro
[2025-08-19T09:29:12.983+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.984+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.984+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.986+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-08-19T09:29:12.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.987+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.987+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.989+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.989+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.990+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:12.990+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:12.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:12.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:12.992+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:12.992+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:12.992+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:12.993+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:12.993+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:12.993+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:12.994+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:12.994+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:12.994+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:12.994+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 13) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00011-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:12.995+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:12.995+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:12.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:12.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:12.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:12.996+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:12.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
[2025-08-19T09:29:12.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:12.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:12.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:12.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:12.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:12.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:12.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:12.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:13.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:13.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:13.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:13.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:13.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:13.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:13.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:13.002+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:13.002+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:13.002+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:13.002+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:13.003+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:13.003+0000] {subprocess.py:93} INFO - 25/08/19 09:29:12 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 12) (172.18.0.16 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:13.003+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:13.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:13.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:13.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:13.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:13.005+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:13.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
[2025-08-19T09:29:13.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:13.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:13.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:13.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:13.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:13.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:13.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:13.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:13.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:13.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:13.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:13.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:13.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:13.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:13.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:13.011+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:13.011+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:13.011+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:13.011+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:13.012+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:13.066+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO TaskSetManager: Starting task 1.1 in stage 1.0 (TID 24) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:13.074+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO TaskSetManager: Lost task 1.1 in stage 3.0 (TID 18) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:13.094+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:13.111+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 25) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:13.209+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 19) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:13.211+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:13.319+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.12:33395 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:13.404+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.12:33395 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:13.456+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.16:34637 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:13.515+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.16:34637 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:13.753+0000] {subprocess.py:93} INFO - 25/08/19 09:29:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.12:33395 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-19T09:29:14.170+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 26) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:14.229+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Starting task 1.2 in stage 3.0 (TID 27) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:14.239+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 20) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00005-30297bc4-c39a-4c53-a779-23eeed37967e-c000.avro
[2025-08-19T09:29:14.240+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:14.240+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:14.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:14.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:14.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:14.256+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:14.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
[2025-08-19T09:29:14.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:14.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:14.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:14.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:14.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:14.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:14.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:14.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:14.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:14.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:14.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:14.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:14.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:14.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:14.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:14.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:14.306+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:14.306+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:14.310+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:14.312+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:14.313+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.16:34637 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:14.325+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 21) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00011-30297bc4-c39a-4c53-a779-23eeed37967e-c000.avro
[2025-08-19T09:29:14.326+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:14.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:14.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:14.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:14.336+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:14.341+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:14.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
[2025-08-19T09:29:14.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:14.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:14.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:14.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:14.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:14.361+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:14.367+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:14.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:14.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:14.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:14.369+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:14.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:14.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:14.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:14.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:14.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:14.393+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:14.399+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:14.399+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:14.400+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:14.487+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.16:34637 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:14.497+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.12:33395 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:14.528+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.12:33395 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:14.715+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Starting task 0.1 in stage 6.0 (TID 28) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:14.774+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Starting task 1.1 in stage 6.0 (TID 29) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:14.795+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 22) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:14.795+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:14.817+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 23) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00005-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:14.818+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:14.877+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 30) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:14.879+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 25) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:14.880+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:14.940+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 31) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:14.985+0000] {subprocess.py:93} INFO - 25/08/19 09:29:14 INFO TaskSetManager: Lost task 1.1 in stage 1.0 (TID 24) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:14.985+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:15.088+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 1.2 in stage 1.0 (TID 32) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:15.149+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 33) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:15.167+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 16) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:15.168+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:15.181+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 17) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:15.183+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:15.381+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 34) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:15.515+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 0.1 in stage 6.0 (TID 28) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:15.519+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:15.531+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 35) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:15.532+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 0.2 in stage 6.0 (TID 36) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:15.543+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 1.1 in stage 6.0 (TID 29) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00011-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:15.543+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:15.619+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 31) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:15.625+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:15.667+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 37) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:15.741+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.12:33395 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:15.741+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 1.2 in stage 6.0 (TID 38) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:15.758+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 30) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00005-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:15.758+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:15.767+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 33) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:15.798+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:15.800+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Lost task 1.2 in stage 1.0 (TID 32) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:15.804+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:15.844+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 39) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:16.004+0000] {subprocess.py:93} INFO - 25/08/19 09:29:15 INFO TaskSetManager: Starting task 1.3 in stage 1.0 (TID 40) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:16.064+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Lost task 1.3 in stage 0.0 (TID 37) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:16.067+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:16.099+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 ERROR TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job
[2025-08-19T09:29:16.121+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 41) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:16.122+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Starting task 1.1 in stage 7.0 (TID 42) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:16.150+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 35) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:16.150+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:16.198+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Lost task 1.2 in stage 2.0 (TID 34) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:16.199+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:16.366+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 43) (172.18.0.16, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:16.387+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Lost task 0.2 in stage 6.0 (TID 36) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:16.390+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:16.440+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-08-19T09:29:16.441+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 37) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:16.477+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:16.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:16.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:16.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:16.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:16.502+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:16.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
[2025-08-19T09:29:16.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:16.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:16.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:16.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:16.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:16.537+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:16.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:16.548+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:16.548+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:16.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:16.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:16.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:16.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:16.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:16.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:16.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:16.602+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:16.602+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:16.603+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:16.634+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:16.640+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-19T09:29:16.822+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSchedulerImpl: Stage 0 was cancelled
[2025-08-19T09:29:16.857+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO DAGScheduler: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.740 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 37) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:16.858+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:16.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:16.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:16.864+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:16.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:16.886+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:16.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
[2025-08-19T09:29:16.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:16.916+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:16.917+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:16.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:16.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:16.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:16.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:16.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:17.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:17.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:17.002+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:17.002+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:17.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:17.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:17.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:17.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:17.037+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:17.038+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:17.038+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:17.038+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:17.059+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-19T09:29:17.072+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 44) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:17.072+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Starting task 0.3 in stage 6.0 (TID 45) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:17.073+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 26) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:17.075+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:17.076+0000] {subprocess.py:93} INFO - 25/08/19 09:29:16 INFO TaskSetManager: Lost task 1.2 in stage 6.0 (TID 38) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00011-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:17.077+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:17.131+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID 46) (172.18.0.12, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:17.208+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Job 0 failed: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 415.912765 s
[2025-08-19T09:29:17.259+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Starting task 1.3 in stage 6.0 (TID 47) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:17.267+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 39) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00005-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:17.268+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.296+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-08-19T09:29:17.311+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 1.2 in stage 3.0 (TID 27) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:17.312+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-19T09:29:17.416+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Starting task 1.3 in stage 3.0 (TID 48) (172.18.0.12, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-19T09:29:17.452+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 1.3 in stage 1.0 (TID 40) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:17.452+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.475+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 ERROR TaskSetManager: Task 1 in stage 1.0 failed 4 times; aborting job
[2025-08-19T09:29:17.506+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Starting task 0.1 in stage 7.0 (TID 49) (172.18.0.16, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-19T09:29:17.531+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 1
[2025-08-19T09:29:17.532+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 40) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:17.535+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:17.536+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:17.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:17.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:17.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:17.559+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:17.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-19T09:29:17.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:17.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:17.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:17.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:17.589+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:17.589+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:17.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:17.591+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:17.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:17.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:17.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:17.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:17.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:17.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:17.596+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:17.596+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:17.596+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:17.597+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:17.598+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:17.598+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:17.599+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-19T09:29:17.599+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 1 was cancelled
[2025-08-19T09:29:17.599+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.484 s due to Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 40) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:17.600+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:17.600+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:17.601+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:17.602+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:17.602+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:17.602+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:17.603+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-19T09:29:17.603+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:17.604+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:17.605+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:17.605+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:17.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:17.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:17.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:17.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:17.608+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:17.608+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:17.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:17.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:17.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:17.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:17.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:17.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:17.611+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:17.612+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:17.612+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:17.612+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:17.613+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-19T09:29:17.613+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId b1846bb4-7dde-4c98-8cf4-44faaf97c161)
[2025-08-19T09:29:17.613+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 9ac861db-cbbc-44a1-a7b6-e0133ad292b2)
[2025-08-19T09:29:17.614+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId c119bd43-5d75-4b84-b3d5-11d60edcdb1f)
[2025-08-19T09:29:17.614+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId f774c1fe-0c39-4ea6-8967-c2e9b493ff6a)
[2025-08-19T09:29:17.615+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 5b2758d8-5285-41ff-ad10-41d258e56853)
[2025-08-19T09:29:17.615+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId b29c5f75-1163-4b15-8863-337cafbd681f)
[2025-08-19T09:29:17.615+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId a79932da-033a-4ec5-a48b-817f4487778b)
[2025-08-19T09:29:17.616+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 890b07d1-e75a-43cc-aae3-3065104b284d)
[2025-08-19T09:29:17.616+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 2e2238b1-c0df-4b74-a4da-b712402afb9c)
[2025-08-19T09:29:17.616+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 7
[2025-08-19T09:29:17.617+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage cancelled: Job 7 cancelled part of cancelled job tag broadcast exchange (runId b1846bb4-7dde-4c98-8cf4-44faaf97c161)
[2025-08-19T09:29:17.617+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 7 was cancelled
[2025-08-19T09:29:17.617+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 413.977 s due to Job 7 cancelled part of cancelled job tag broadcast exchange (runId b1846bb4-7dde-4c98-8cf4-44faaf97c161)
[2025-08-19T09:29:17.618+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 3
[2025-08-19T09:29:17.618+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled: Job 3 cancelled part of cancelled job tag broadcast exchange (runId 9ac861db-cbbc-44a1-a7b6-e0133ad292b2)
[2025-08-19T09:29:17.618+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 3 was cancelled
[2025-08-19T09:29:17.619+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.086 s due to Job 3 cancelled part of cancelled job tag broadcast exchange (runId 9ac861db-cbbc-44a1-a7b6-e0133ad292b2)
[2025-08-19T09:29:17.619+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 6
[2025-08-19T09:29:17.619+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage cancelled: Job 6 cancelled part of cancelled job tag broadcast exchange (runId c119bd43-5d75-4b84-b3d5-11d60edcdb1f)
[2025-08-19T09:29:17.620+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 6 was cancelled
[2025-08-19T09:29:17.620+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.017 s due to Job 6 cancelled part of cancelled job tag broadcast exchange (runId c119bd43-5d75-4b84-b3d5-11d60edcdb1f)
[2025-08-19T09:29:17.620+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 5
[2025-08-19T09:29:17.621+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job 5 cancelled part of cancelled job tag broadcast exchange (runId 5b2758d8-5285-41ff-ad10-41d258e56853)
[2025-08-19T09:29:17.621+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 5 was cancelled
[2025-08-19T09:29:17.621+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.036 s due to Job 5 cancelled part of cancelled job tag broadcast exchange (runId 5b2758d8-5285-41ff-ad10-41d258e56853)
[2025-08-19T09:29:17.622+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 9
[2025-08-19T09:29:17.622+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage cancelled: Job 8 cancelled part of cancelled job tag broadcast exchange (runId b29c5f75-1163-4b15-8863-337cafbd681f)
[2025-08-19T09:29:17.623+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-08-19T09:29:17.623+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 9 was cancelled
[2025-08-19T09:29:17.623+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 413.732 s due to Job 8 cancelled part of cancelled job tag broadcast exchange (runId b29c5f75-1163-4b15-8863-337cafbd681f)
[2025-08-19T09:29:17.624+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 8
[2025-08-19T09:29:17.624+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId a79932da-033a-4ec5-a48b-817f4487778b)
[2025-08-19T09:29:17.625+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-08-19T09:29:17.625+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 8 was cancelled
[2025-08-19T09:29:17.626+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 413.928 s due to Job 9 cancelled part of cancelled job tag broadcast exchange (runId a79932da-033a-4ec5-a48b-817f4487778b)
[2025-08-19T09:29:17.626+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 1.1 in stage 7.0 (TID 42) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-05b9d236-db8d-4e1a-a709-8449bc2fd6ef-c000.avro
[2025-08-19T09:29:17.627+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-19T09:29:17.644+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 41) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-e33ab6c7-b0fc-4af9-aa8d-56ba34ebe609-c000.avro
[2025-08-19T09:29:17.644+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.649+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-08-19T09:29:17.650+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.12:33395 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-19T09:29:17.651+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 43) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:17.651+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.673+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job
[2025-08-19T09:29:17.688+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-08-19T09:29:17.694+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 4
[2025-08-19T09:29:17.695+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled: Job 4 cancelled part of cancelled job tag broadcast exchange (runId 890b07d1-e75a-43cc-aae3-3065104b284d)
[2025-08-19T09:29:17.698+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Stage 4 was cancelled
[2025-08-19T09:29:17.714+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.189 s due to Job 4 cancelled part of cancelled job tag broadcast exchange (runId 890b07d1-e75a-43cc-aae3-3065104b284d)
[2025-08-19T09:29:17.715+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Cancelling stage 2
[2025-08-19T09:29:17.718+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job 2 cancelled part of cancelled job tag broadcast exchange (runId 2e2238b1-c0df-4b74-a4da-b712402afb9c)
[2025-08-19T09:29:17.719+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 414.384 s due to Job 2 cancelled part of cancelled job tag broadcast exchange (runId 2e2238b1-c0df-4b74-a4da-b712402afb9c)
[2025-08-19T09:29:17.726+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 0.3 in stage 6.0 (TID 45) on 172.18.0.16, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-525a7980-fc82-49cf-910b-09478765fe24-c000.avro
[2025-08-19T09:29:17.727+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.750+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 44) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-3901b8d9-c898-428f-bc72-8a4f18a9d76b-c000.avro
[2025-08-19T09:29:17.753+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.754+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-08-19T09:29:17.754+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 46) on 172.18.0.12, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-7c7dcf6c-fc69-4763-84df-998351558e86-c000.avro
[2025-08-19T09:29:17.755+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-19T09:29:17.897+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 8) (172.18.0.17 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-f042fc66-b31a-4b25-887a-e1f32eba4a1c-c000.avro
[2025-08-19T09:29:17.898+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:17.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:17.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:17.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:17.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:17.900+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:17.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-19T09:29:17.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:17.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:17.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:17.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:17.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:17.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:17.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:17.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:17.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:17.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:17.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:17.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:17.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:17.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:17.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:17.960+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:17.961+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:17.965+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:17.966+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:17.966+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:17.966+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 10) (172.18.0.17 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-ecbdf5f1-0a72-4fcd-b78e-5f8f515f62a0-c000.avro
[2025-08-19T09:29:17.966+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:17.967+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:17.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:17.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:17.983+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:17.989+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:17.993+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-19T09:29:17.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:17.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:18.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:18.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:18.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:18.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:18.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:18.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:18.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:18.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:18.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:18.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:18.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:18.033+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:18.033+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:18.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:18.034+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:18.034+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:18.035+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:18.035+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:18.051+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 11) (172.18.0.17 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-ecbdf5f1-0a72-4fcd-b78e-5f8f515f62a0-c000.avro
[2025-08-19T09:29:18.055+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:18.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:18.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:18.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:18.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:18.072+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:18.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-19T09:29:18.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:18.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:18.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:18.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:18.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:18.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:18.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:18.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:18.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:18.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:18.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:18.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:18.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:18.138+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:18.138+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:18.139+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:18.145+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:18.145+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:18.146+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:18.147+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:18.147+0000] {subprocess.py:93} INFO - 25/08/19 09:29:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-08-19T09:29:18.165+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 9) (172.18.0.17 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-f042fc66-b31a-4b25-887a-e1f32eba4a1c-c000.avro
[2025-08-19T09:29:18.165+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:18.165+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:18.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:18.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:18.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:18.169+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:18.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-19T09:29:18.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:18.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:18.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:18.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:18.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:18.203+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:18.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:18.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:18.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:18.205+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:18.215+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:18.217+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:18.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:18.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:18.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:18.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:18.232+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:18.233+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:18.233+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:18.233+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:18.234+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-08-19T09:29:18.235+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 WARN TaskSetManager: Lost task 0.1 in stage 7.0 (TID 49) (172.18.0.16 executor 2): TaskKilled (Stage cancelled: Job 7 cancelled part of cancelled job tag broadcast exchange (runId b1846bb4-7dde-4c98-8cf4-44faaf97c161))
[2025-08-19T09:29:18.251+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-08-19T09:29:18.413+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 WARN TaskSetManager: Lost task 1.3 in stage 3.0 (TID 48) (172.18.0.12 executor 0): TaskKilled (Stage cancelled: Job 3 cancelled part of cancelled job tag broadcast exchange (runId 9ac861db-cbbc-44a1-a7b6-e0133ad292b2))
[2025-08-19T09:29:18.413+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-08-19T09:29:18.518+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 WARN TaskSetManager: Lost task 1.3 in stage 6.0 (TID 47) (172.18.0.12 executor 0): TaskKilled (Stage cancelled: Job 6 cancelled part of cancelled job tag broadcast exchange (runId c119bd43-5d75-4b84-b3d5-11d60edcdb1f))
[2025-08-19T09:29:18.526+0000] {subprocess.py:93} INFO - 25/08/19 09:29:18 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-08-19T09:29:19.963+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-08-19T09:29:19.966+0000] {subprocess.py:93} INFO -   File "/opt/***/jobs/transformation/transform_to_golden_dataset.py", line 135, in <module>
[2025-08-19T09:29:19.967+0000] {subprocess.py:93} INFO -     final_df.write.format("parquet").mode("overwrite").save("hdfs://namenode:9000/storage/hdfs/processed/golden_dataset")
[2025-08-19T09:29:19.967+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1463, in save
[2025-08-19T09:29:19.967+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-08-19T09:29:19.968+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-08-19T09:29:19.969+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-08-19T09:29:19.969+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o1969.save.
[2025-08-19T09:29:19.969+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 37) (172.18.0.12 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:19.970+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:19.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:19.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:19.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:19.971+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:19.973+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:19.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
[2025-08-19T09:29:19.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:19.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:19.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:19.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:19.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:19.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:19.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:19.978+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:19.979+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:19.979+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:19.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:19.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:19.981+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:19.981+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:19.981+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:19.982+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:19.984+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:19.985+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:19.985+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:19.986+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:19.987+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-19T09:29:19.987+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-08-19T09:29:19.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-08-19T09:29:19.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-08-19T09:29:19.989+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-08-19T09:29:19.992+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-08-19T09:29:19.993+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-08-19T09:29:19.993+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-08-19T09:29:19.994+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-08-19T09:29:19.995+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-08-19T09:29:19.995+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-08-19T09:29:19.995+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-08-19T09:29:19.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-08-19T09:29:19.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-08-19T09:29:19.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-08-19T09:29:19.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-08-19T09:29:19.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-08-19T09:29:19.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-08-19T09:29:19.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-08-19T09:29:20.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-08-19T09:29:20.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
[2025-08-19T09:29:20.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
[2025-08-19T09:29:20.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-08-19T09:29:20.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2025-08-19T09:29:20.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
[2025-08-19T09:29:20.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
[2025-08-19T09:29:20.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)
[2025-08-19T09:29:20.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)
[2025-08-19T09:29:20.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)
[2025-08-19T09:29:20.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-19T09:29:20.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)
[2025-08-19T09:29:20.008+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2025-08-19T09:29:20.009+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-19T09:29:20.009+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-19T09:29:20.009+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-19T09:29:20.009+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/IN_category_id/part-00011-7b81abc2-906d-48da-8b10-371ef1335ac4-c000.avro
[2025-08-19T09:29:20.010+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-19T09:29:20.011+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-19T09:29:20.012+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-19T09:29:20.012+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-19T09:29:20.012+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-19T09:29:20.013+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-19T09:29:20.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
[2025-08-19T09:29:20.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-19T09:29:20.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-19T09:29:20.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-19T09:29:20.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-19T09:29:20.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-19T09:29:20.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-19T09:29:20.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-19T09:29:20.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-19T09:29:20.033+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-19T09:29:20.036+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-19T09:29:20.037+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-19T09:29:20.040+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-19T09:29:20.040+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-19T09:29:20.041+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-19T09:29:20.044+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-19T09:29:20.045+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-19T09:29:20.045+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-08-19T09:29:20.046+0000] {subprocess.py:93} INFO - 
[2025-08-19T09:29:20.131+0000] {subprocess.py:93} INFO - 25/08/19 09:29:20 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-19T09:29:20.132+0000] {subprocess.py:93} INFO - 25/08/19 09:29:20 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-08-19T09:29:20.201+0000] {subprocess.py:93} INFO - 25/08/19 09:29:20 INFO SparkUI: Stopped Spark web UI at http://b208641feb52:4041
[2025-08-19T09:29:20.227+0000] {subprocess.py:93} INFO - 25/08/19 09:29:20 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-19T09:29:20.230+0000] {subprocess.py:93} INFO - 25/08/19 09:29:20 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-19T09:29:20.851+0000] {subprocess.py:93} INFO - 25/08/19 09:29:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-19T09:29:21.362+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO MemoryStore: MemoryStore cleared
[2025-08-19T09:29:21.368+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO BlockManager: BlockManager stopped
[2025-08-19T09:29:21.550+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-19T09:29:21.636+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-19T09:29:21.881+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO SparkContext: Successfully stopped SparkContext
[2025-08-19T09:29:21.883+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO ShutdownHookManager: Shutdown hook called
[2025-08-19T09:29:21.937+0000] {subprocess.py:93} INFO - 25/08/19 09:29:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-4d32c31f-c646-4a62-98b0-5ccd58ca2c08/pyspark-c479f067-f344-4a49-8469-d2bbc6c40a02
[2025-08-19T09:29:22.170+0000] {subprocess.py:93} INFO - 25/08/19 09:29:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-4d32c31f-c646-4a62-98b0-5ccd58ca2c08
[2025-08-19T09:29:22.267+0000] {subprocess.py:93} INFO - 25/08/19 09:29:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-60b145f6-e481-44eb-b6c2-fadf2b4a56a2
[2025-08-19T09:29:23.974+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-08-19T09:29:23.976+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-08-19T09:29:24.001+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-08-19T09:29:24.008+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=youtube_pipeline_batch_data, task_id=transform_golden, execution_date=20250819T091113, start_date=20250819T092042, end_date=20250819T092924
[2025-08-19T09:29:24.067+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 600 for task transform_golden (Bash command failed. The command returned a non-zero exit code 1.; 2287)
[2025-08-19T09:29:24.132+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-08-19T09:29:24.600+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-19T09:29:24.648+0000] {local_task_job_runner.py:222} INFO - ::endgroup::

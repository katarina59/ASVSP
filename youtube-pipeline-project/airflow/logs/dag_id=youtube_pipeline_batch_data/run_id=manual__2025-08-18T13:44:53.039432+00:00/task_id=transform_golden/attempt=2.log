[2025-08-18T13:48:52.268+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-08-18T13:48:52.390+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-18T13:44:53.039432+00:00 [queued]>
[2025-08-18T13:48:52.410+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-18T13:44:53.039432+00:00 [queued]>
[2025-08-18T13:48:52.410+0000] {taskinstance.py:2303} INFO - Starting attempt 2 of 4
[2025-08-18T13:48:52.445+0000] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): transform_golden> on 2025-08-18 13:44:53.039432+00:00
[2025-08-18T13:48:52.456+0000] {standard_task_runner.py:63} INFO - Started process 5454 to run task
[2025-08-18T13:48:52.466+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'youtube_pipeline_batch_data', 'transform_golden', 'manual__2025-08-18T13:44:53.039432+00:00', '--job-id', '349', '--raw', '--subdir', 'DAGS_FOLDER/youtube_pipeline.py', '--cfg-path', '/tmp/tmpnjokrnh1']
[2025-08-18T13:48:52.469+0000] {standard_task_runner.py:91} INFO - Job 349: Subtask transform_golden
[2025-08-18T13:48:52.603+0000] {task_command.py:426} INFO - Running <TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-18T13:44:53.039432+00:00 [running]> on host 47205959681c
[2025-08-18T13:48:52.937+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='youtube_pipeline_batch_data' AIRFLOW_CTX_TASK_ID='transform_golden' AIRFLOW_CTX_EXECUTION_DATE='2025-08-18T13:44:53.039432+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-18T13:44:53.039432+00:00'
[2025-08-18T13:48:52.940+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-08-18T13:48:53.013+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-08-18T13:48:53.016+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'docker exec youtube-pipeline-project_spark-master_1 spark-submit --master spark://spark-master:7077 --deploy-mode client --packages org.apache.spark:spark-avro_2.12:3.5.1 /opt/***/jobs/transformation/transform_to_golden_dataset.py']
[2025-08-18T13:48:53.058+0000] {subprocess.py:86} INFO - Output:
[2025-08-18T13:48:58.897+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-08-18T13:48:59.234+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2025-08-18T13:48:59.236+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-08-18T13:48:59.277+0000] {subprocess.py:93} INFO - org.apache.spark#spark-avro_2.12 added as a dependency
[2025-08-18T13:48:59.281+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-7441f897-e59e-480e-84fe-6a9488c4a4e9;1.0
[2025-08-18T13:48:59.286+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-08-18T13:49:00.483+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-avro_2.12;3.5.1 in central
[2025-08-18T13:49:00.527+0000] {subprocess.py:93} INFO - 	found org.tukaani#xz;1.9 in central
[2025-08-18T13:49:00.576+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1271ms :: artifacts dl 24ms
[2025-08-18T13:49:00.578+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-08-18T13:49:00.579+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-avro_2.12;3.5.1 from central in [default]
[2025-08-18T13:49:00.580+0000] {subprocess.py:93} INFO - 	org.tukaani#xz;1.9 from central in [default]
[2025-08-18T13:49:00.580+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-18T13:49:00.581+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-08-18T13:49:00.581+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-08-18T13:49:00.581+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-18T13:49:00.586+0000] {subprocess.py:93} INFO - 	|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
[2025-08-18T13:49:00.590+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-18T13:49:00.591+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-7441f897-e59e-480e-84fe-6a9488c4a4e9
[2025-08-18T13:49:00.591+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-08-18T13:49:00.595+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 2 already retrieved (0kB/13ms)
[2025-08-18T13:49:01.019+0000] {subprocess.py:93} INFO - 25/08/18 13:49:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-18T13:49:04.050+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SparkContext: Running Spark version 3.5.1
[2025-08-18T13:49:04.052+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SparkContext: OS info Linux, 6.11.0-29-generic, amd64
[2025-08-18T13:49:04.054+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SparkContext: Java version 17.0.12
[2025-08-18T13:49:04.107+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO ResourceUtils: ==============================================================
[2025-08-18T13:49:04.110+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-18T13:49:04.111+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO ResourceUtils: ==============================================================
[2025-08-18T13:49:04.113+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SparkContext: Submitted application: Processed Zone - Golden Dataset
[2025-08-18T13:49:04.424+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-18T13:49:04.493+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO ResourceProfile: Limiting resource is cpu
[2025-08-18T13:49:04.497+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-18T13:49:04.720+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SecurityManager: Changing view acls to: spark
[2025-08-18T13:49:04.722+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SecurityManager: Changing modify acls to: spark
[2025-08-18T13:49:04.732+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SecurityManager: Changing view acls groups to:
[2025-08-18T13:49:04.735+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SecurityManager: Changing modify acls groups to:
[2025-08-18T13:49:04.737+0000] {subprocess.py:93} INFO - 25/08/18 13:49:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-08-18T13:49:06.113+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO Utils: Successfully started service 'sparkDriver' on port 43799.
[2025-08-18T13:49:06.143+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkEnv: Registering MapOutputTracker
[2025-08-18T13:49:06.186+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-18T13:49:06.204+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-18T13:49:06.205+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-18T13:49:06.211+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-18T13:49:06.247+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e8607794-52c6-42bc-80c5-37d7edf7df71
[2025-08-18T13:49:06.262+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-18T13:49:06.280+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-18T13:49:06.552+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-18T13:49:06.725+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-08-18T13:49:06.855+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar at spark://4bf6b336ccb3:43799/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar with timestamp 1755524944035
[2025-08-18T13:49:06.866+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://4bf6b336ccb3:43799/jars/org.tukaani_xz-1.9.jar with timestamp 1755524944035
[2025-08-18T13:49:06.881+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar at spark://4bf6b336ccb3:43799/files/org.apache.spark_spark-avro_2.12-3.5.1.jar with timestamp 1755524944035
[2025-08-18T13:49:06.919+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar to /tmp/spark-abd46729-5ba9-4fc9-a9e3-1156733c9af4/userFiles-32e06918-1720-48ac-9587-cf8bc81b12ab/org.apache.spark_spark-avro_2.12-3.5.1.jar
[2025-08-18T13:49:06.970+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://4bf6b336ccb3:43799/files/org.tukaani_xz-1.9.jar with timestamp 1755524944035
[2025-08-18T13:49:06.972+0000] {subprocess.py:93} INFO - 25/08/18 13:49:06 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar to /tmp/spark-abd46729-5ba9-4fc9-a9e3-1156733c9af4/userFiles-32e06918-1720-48ac-9587-cf8bc81b12ab/org.tukaani_xz-1.9.jar
[2025-08-18T13:49:07.379+0000] {subprocess.py:93} INFO - 25/08/18 13:49:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-18T13:49:07.658+0000] {subprocess.py:93} INFO - 25/08/18 13:49:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.7:7077 after 201 ms (0 ms spent in bootstraps)
[2025-08-18T13:49:08.107+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250818134908-0042
[2025-08-18T13:49:08.154+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41693.
[2025-08-18T13:49:08.155+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO NettyBlockTransferService: Server created on 4bf6b336ccb3:41693
[2025-08-18T13:49:08.163+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-18T13:49:08.182+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4bf6b336ccb3, 41693, None)
[2025-08-18T13:49:08.191+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO BlockManagerMasterEndpoint: Registering block manager 4bf6b336ccb3:41693 with 434.4 MiB RAM, BlockManagerId(driver, 4bf6b336ccb3, 41693, None)
[2025-08-18T13:49:08.201+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4bf6b336ccb3, 41693, None)
[2025-08-18T13:49:08.209+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4bf6b336ccb3, 41693, None)
[2025-08-18T13:49:08.630+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-18T13:49:08.894+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-18T13:49:08.901+0000] {subprocess.py:93} INFO - 25/08/18 13:49:08 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2025-08-18T13:49:13.263+0000] {subprocess.py:93} INFO - 25/08/18 13:49:13 INFO InMemoryFileIndex: It took 666 ms to list leaf files for 1 paths.
[2025-08-18T13:49:16.927+0000] {subprocess.py:93} INFO - 25/08/18 13:49:16 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.
[2025-08-18T13:49:20.735+0000] {subprocess.py:93} INFO - 25/08/18 13:49:20 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2025-08-18T13:49:20.825+0000] {subprocess.py:93} INFO - 25/08/18 13:49:20 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
[2025-08-18T13:49:22.622+0000] {subprocess.py:93} INFO - 25/08/18 13:49:22 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.
[2025-08-18T13:49:22.667+0000] {subprocess.py:93} INFO - 25/08/18 13:49:22 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
[2025-08-18T13:49:24.445+0000] {subprocess.py:93} INFO - 25/08/18 13:49:24 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2025-08-18T13:49:24.526+0000] {subprocess.py:93} INFO - 25/08/18 13:49:24 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
[2025-08-18T13:49:26.085+0000] {subprocess.py:93} INFO - 25/08/18 13:49:26 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
[2025-08-18T13:49:26.312+0000] {subprocess.py:93} INFO - 25/08/18 13:49:26 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
[2025-08-18T13:49:27.525+0000] {subprocess.py:93} INFO - 25/08/18 13:49:27 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2025-08-18T13:49:27.673+0000] {subprocess.py:93} INFO - 25/08/18 13:49:27 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2025-08-18T13:49:28.456+0000] {subprocess.py:93} INFO - 25/08/18 13:49:28 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.
[2025-08-18T13:49:28.506+0000] {subprocess.py:93} INFO - 25/08/18 13:49:28 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2025-08-18T13:49:30.213+0000] {subprocess.py:93} INFO - 25/08/18 13:49:30 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
[2025-08-18T13:49:30.265+0000] {subprocess.py:93} INFO - 25/08/18 13:49:30 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
[2025-08-18T13:49:31.211+0000] {subprocess.py:93} INFO - 25/08/18 13:49:31 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.
[2025-08-18T13:49:31.372+0000] {subprocess.py:93} INFO - 25/08/18 13:49:31 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.
[2025-08-18T13:49:32.877+0000] {subprocess.py:93} INFO - 25/08/18 13:49:32 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2025-08-18T13:49:33.106+0000] {subprocess.py:93} INFO - 25/08/18 13:49:33 INFO InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.
[2025-08-18T13:49:43.274+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.335+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#7),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None) END),(cast(views#7 as bigint) > 0)
[2025-08-18T13:49:43.470+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.500+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#32)
[2025-08-18T13:49:43.638+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.646+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#500),isnotnull(CASE WHEN (length(trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None) END),(cast(views#500 as bigint) > 0)
[2025-08-18T13:49:43.651+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.656+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#525)
[2025-08-18T13:49:43.704+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.707+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#993),isnotnull(CASE WHEN (length(trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None) END),(cast(views#993 as bigint) > 0)
[2025-08-18T13:49:43.711+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.712+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#1018)
[2025-08-18T13:49:43.726+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.731+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#1486),isnotnull(CASE WHEN (length(trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None) END),(cast(views#1486 as bigint) > 0)
[2025-08-18T13:49:43.733+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.734+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#1511)
[2025-08-18T13:49:43.744+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.745+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#1979),isnotnull(CASE WHEN (length(trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None) END),(cast(views#1979 as bigint) > 0)
[2025-08-18T13:49:43.747+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.752+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2004)
[2025-08-18T13:49:43.764+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.768+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#2472),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None) END),(cast(views#2472 as bigint) > 0)
[2025-08-18T13:49:43.771+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.772+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2497)
[2025-08-18T13:49:43.800+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.809+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#2965),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None) END),(cast(views#2965 as bigint) > 0)
[2025-08-18T13:49:43.810+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.821+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2990)
[2025-08-18T13:49:43.829+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.834+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#3458),isnotnull(CASE WHEN (length(trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None) END),(cast(views#3458 as bigint) > 0)
[2025-08-18T13:49:43.844+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.851+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#3483)
[2025-08-18T13:49:43.865+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.869+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#3951),isnotnull(CASE WHEN (length(trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None) END),(cast(views#3951 as bigint) > 0)
[2025-08-18T13:49:43.870+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.875+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#3976)
[2025-08-18T13:49:43.917+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:49:43.919+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#4444),isnotnull(CASE WHEN (length(trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None) END),(cast(views#4444 as bigint) > 0)
[2025-08-18T13:49:43.928+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:49:43.929+0000] {subprocess.py:93} INFO - 25/08/18 13:49:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#4469)
[2025-08-18T13:49:46.207+0000] {subprocess.py:93} INFO - 25/08/18 13:49:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-08-18T13:49:49.947+0000] {subprocess.py:93} INFO - 25/08/18 13:49:49 INFO CodeGenerator: Code generated in 1139.042759 ms
[2025-08-18T13:49:49.954+0000] {subprocess.py:93} INFO - 25/08/18 13:49:49 INFO CodeGenerator: Code generated in 1164.43426 ms
[2025-08-18T13:49:49.957+0000] {subprocess.py:93} INFO - 25/08/18 13:49:49 INFO CodeGenerator: Code generated in 1216.101969 ms
[2025-08-18T13:49:49.969+0000] {subprocess.py:93} INFO - 25/08/18 13:49:49 INFO CodeGenerator: Code generated in 1164.594213 ms
[2025-08-18T13:49:49.974+0000] {subprocess.py:93} INFO - 25/08/18 13:49:49 INFO CodeGenerator: Code generated in 1172.604028 ms
[2025-08-18T13:49:50.069+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO CodeGenerator: Code generated in 1260.026294 ms
[2025-08-18T13:49:50.085+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 200.9 KiB, free 434.2 MiB)
[2025-08-18T13:49:50.101+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO CodeGenerator: Code generated in 1269.582109 ms
[2025-08-18T13:49:50.120+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO CodeGenerator: Code generated in 1299.194899 ms
[2025-08-18T13:49:50.132+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO CodeGenerator: Code generated in 1340.462381 ms
[2025-08-18T13:49:50.138+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.9 KiB, free 433.8 MiB)
[2025-08-18T13:49:50.142+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.9 KiB, free 433.4 MiB)
[2025-08-18T13:49:50.153+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.9 KiB, free 434.0 MiB)
[2025-08-18T13:49:50.154+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO CodeGenerator: Code generated in 1309.717376 ms
[2025-08-18T13:49:50.170+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.9 KiB, free 433.6 MiB)
[2025-08-18T13:49:50.223+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.9 KiB, free 433.2 MiB)
[2025-08-18T13:49:50.249+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.9 KiB, free 433.0 MiB)
[2025-08-18T13:49:50.278+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.6 MiB)
[2025-08-18T13:49:50.291+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T13:49:50.297+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.9 KiB, free 432.8 MiB)
[2025-08-18T13:49:50.310+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.9 KiB, free 432.6 MiB)
[2025-08-18T13:49:50.330+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.341+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.9 KiB, free 432.4 MiB)
[2025-08-18T13:49:50.343+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167907 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.422+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.4 MiB)
[2025-08-18T13:49:50.424+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:49:50.428+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-18T13:49:50.428+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:49:50.436+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.439+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.440+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.441+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.463+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-18T13:49:50.464+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:49:50.467+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 1 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.468+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.516+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-18T13:49:50.517+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:49:50.522+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.524+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.626+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-18T13:49:50.628+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:49:50.633+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.634+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.775+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-18T13:49:50.826+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-18T13:49:50.839+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:49:50.848+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:49:50.858+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-18T13:49:50.862+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.864+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:49:50.868+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-18T13:49:50.869+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.870+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.896+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4bf6b336ccb3:41693 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:49:50.909+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12583966 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.919+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.922+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:50.926+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:50.945+0000] {subprocess.py:93} INFO - 25/08/18 13:49:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:49:51.435+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.457+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.469+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.486+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.498+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.513+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.518+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.528+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.540+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.541+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:49:51.590+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO DAGScheduler: Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:51.592+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:51.594+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:51.610+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:51.636+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:51.748+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.1 KiB, free 432.1 MiB)
[2025-08-18T13:49:51.769+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.1 MiB)
[2025-08-18T13:49:51.773+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T13:49:51.777+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:51.938+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:51.974+0000] {subprocess.py:93} INFO - 25/08/18 13:49:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.037+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.039+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.044+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.053+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.059+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.104+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.1 KiB, free 432.1 MiB)
[2025-08-18T13:49:52.107+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.1 MiB)
[2025-08-18T13:49:52.109+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.122+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.124+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.126+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.132+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.135+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.140+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.143+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.148+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.155+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.160+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.170+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.177+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.184+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.189+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.192+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.194+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.200+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.202+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.204+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.209+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.389+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.401+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.417+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.419+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.424+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.439+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.441+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.446+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.452+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.458+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.476+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.489+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.491+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.500+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.505+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.506+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.510+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.518+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.520+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.523+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.533+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.553+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.569+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.572+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.575+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.585+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.589+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.600+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.609+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.614+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.619+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.627+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.637+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.649+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:49:52.651+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.655+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.657+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.658+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.666+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.668+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.671+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.672+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.673+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[34] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.689+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-18T13:49:52.694+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-18T13:49:52.695+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.697+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.698+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[34] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.705+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.713+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.714+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.716+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.716+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.722+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.727+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-18T13:49:52.731+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-18T13:49:52.734+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.739+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.740+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.741+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0
[2025-08-18T13:49:52.742+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:49:52.745+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:49:52.747+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:49:52.747+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:49:52.748+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:49:52.755+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-18T13:49:52.759+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-18T13:49:52.761+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 4bf6b336ccb3:41693 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:49:52.763+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:49:52.765+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:49:52.767+0000] {subprocess.py:93} INFO - 25/08/18 13:49:52 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0
[2025-08-18T13:50:07.026+0000] {subprocess.py:93} INFO - 25/08/18 13:50:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:50:22.045+0000] {subprocess.py:93} INFO - 25/08/18 13:50:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:50:37.032+0000] {subprocess.py:93} INFO - 25/08/18 13:50:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:50:52.029+0000] {subprocess.py:93} INFO - 25/08/18 13:50:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:51:07.027+0000] {subprocess.py:93} INFO - 25/08/18 13:51:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:51:22.027+0000] {subprocess.py:93} INFO - 25/08/18 13:51:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:51:37.037+0000] {subprocess.py:93} INFO - 25/08/18 13:51:37 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:51:52.054+0000] {subprocess.py:93} INFO - 25/08/18 13:51:52 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:52:07.026+0000] {subprocess.py:93} INFO - 25/08/18 13:52:07 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:52:10.068+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250818134908-0042/0 on worker-20250818100309-172.18.0.14-34047 (172.18.0.14:34047) with 4 core(s)
[2025-08-18T13:52:10.070+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250818134908-0042/0 on hostPort 172.18.0.14:34047 with 4 core(s), 1024.0 MiB RAM
[2025-08-18T13:52:10.094+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250818134908-0042/1 on worker-20250818100311-172.18.0.13-33759 (172.18.0.13:33759) with 4 core(s)
[2025-08-18T13:52:10.099+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250818134908-0042/1 on hostPort 172.18.0.13:33759 with 4 core(s), 1024.0 MiB RAM
[2025-08-18T13:52:10.104+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250818134908-0042/2 on worker-20250818100314-172.18.0.12-34745 (172.18.0.12:34745) with 4 core(s)
[2025-08-18T13:52:10.114+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250818134908-0042/2 on hostPort 172.18.0.12:34745 with 4 core(s), 1024.0 MiB RAM
[2025-08-18T13:52:10.532+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250818134908-0042/0 is now RUNNING
[2025-08-18T13:52:10.587+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250818134908-0042/2 is now RUNNING
[2025-08-18T13:52:10.699+0000] {subprocess.py:93} INFO - 25/08/18 13:52:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250818134908-0042/1 is now RUNNING
[2025-08-18T13:52:22.036+0000] {subprocess.py:93} INFO - 25/08/18 13:52:22 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:52:29.969+0000] {subprocess.py:93} INFO - 25/08/18 13:52:29 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:41102) with ID 0,  ResourceProfileId 0
[2025-08-18T13:52:30.534+0000] {subprocess.py:93} INFO - 25/08/18 13:52:30 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.13:58624) with ID 1,  ResourceProfileId 0
[2025-08-18T13:52:30.550+0000] {subprocess.py:93} INFO - 25/08/18 13:52:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:32985 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.14, 32985, None)
[2025-08-18T13:52:31.035+0000] {subprocess.py:93} INFO - 25/08/18 13:52:31 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.13:43991 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.13, 43991, None)
[2025-08-18T13:52:31.853+0000] {subprocess.py:93} INFO - 25/08/18 13:52:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:31.912+0000] {subprocess.py:93} INFO - 25/08/18 13:52:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:31.917+0000] {subprocess.py:93} INFO - 25/08/18 13:52:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:31.922+0000] {subprocess.py:93} INFO - 25/08/18 13:52:31 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:32.015+0000] {subprocess.py:93} INFO - 25/08/18 13:52:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:32.018+0000] {subprocess.py:93} INFO - 25/08/18 13:52:32 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:32.032+0000] {subprocess.py:93} INFO - 25/08/18 13:52:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:32.034+0000] {subprocess.py:93} INFO - 25/08/18 13:52:32 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:32.282+0000] {subprocess.py:93} INFO - 25/08/18 13:52:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:51648) with ID 2,  ResourceProfileId 0
[2025-08-18T13:52:33.204+0000] {subprocess.py:93} INFO - 25/08/18 13:52:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:34115 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.12, 34115, None)
[2025-08-18T13:52:33.932+0000] {subprocess.py:93} INFO - 25/08/18 13:52:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:33.936+0000] {subprocess.py:93} INFO - 25/08/18 13:52:33 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:33.947+0000] {subprocess.py:93} INFO - 25/08/18 13:52:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:33.966+0000] {subprocess.py:93} INFO - 25/08/18 13:52:33 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:34.217+0000] {subprocess.py:93} INFO - 25/08/18 13:52:34 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:34.249+0000] {subprocess.py:93} INFO - 25/08/18 13:52:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:34.250+0000] {subprocess.py:93} INFO - 25/08/18 13:52:34 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:34.260+0000] {subprocess.py:93} INFO - 25/08/18 13:52:34 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:35.296+0000] {subprocess.py:93} INFO - 25/08/18 13:52:35 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.12:34115 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:35.306+0000] {subprocess.py:93} INFO - 25/08/18 13:52:35 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.12:34115 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:39.100+0000] {subprocess.py:93} INFO - 25/08/18 13:52:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.13:43991 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:39.232+0000] {subprocess.py:93} INFO - 25/08/18 13:52:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.13:43991 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:39.418+0000] {subprocess.py:93} INFO - 25/08/18 13:52:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:39.438+0000] {subprocess.py:93} INFO - 25/08/18 13:52:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:41.244+0000] {subprocess.py:93} INFO - 25/08/18 13:52:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.12:34115 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T13:52:41.279+0000] {subprocess.py:93} INFO - 25/08/18 13:52:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.12:34115 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:45.859+0000] {subprocess.py:93} INFO - 25/08/18 13:52:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:45.899+0000] {subprocess.py:93} INFO - 25/08/18 13:52:45 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:45.941+0000] {subprocess.py:93} INFO - 25/08/18 13:52:45 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:46.010+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 15) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:46.115+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:46.116+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:46.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:46.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:46.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:46.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:46.118+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:46.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T13:52:46.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:46.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:46.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:46.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:46.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:46.120+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:46.120+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:46.120+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:46.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:46.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:46.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:46.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:46.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:46.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:46.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:46.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:46.124+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:46.124+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:46.125+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:46.128+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:46.211+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 3) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00011-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:46.211+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:46.212+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:46.212+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:46.213+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:46.213+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:46.213+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:46.213+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)
[2025-08-18T13:52:46.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:46.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:46.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:46.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:46.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:46.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:46.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:46.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:46.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:46.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:46.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:46.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:46.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:46.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:46.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:46.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:46.258+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:46.258+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:46.258+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:46.259+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:46.259+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:46.259+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:46.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:46.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:46.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:46.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:46.269+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:46.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T13:52:46.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:46.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:46.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:46.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:46.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:46.320+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:46.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:46.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:46.336+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:46.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:46.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:46.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:46.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:46.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:46.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:46.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:46.364+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:46.365+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:46.365+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:46.365+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:46.371+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00005-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:46.371+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:46.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:46.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:46.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:46.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:46.373+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:46.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)
[2025-08-18T13:52:46.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:46.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:46.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:46.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:46.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:46.394+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:46.394+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:46.394+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:46.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:46.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:46.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:46.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:46.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:46.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:46.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:46.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:46.428+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:46.429+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:46.429+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:46.429+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:46.430+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:46.491+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:46.849+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:46.854+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 16) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:46.872+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:46.901+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:46.903+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:46.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:46.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:46.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:46.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:46.905+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:46.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-18T13:52:46.913+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:46.917+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:46.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:46.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:46.924+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:46.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:46.928+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:46.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:46.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:46.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:46.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:46.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:46.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:46.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:46.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:46.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:46.943+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:46.945+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:46.946+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:46.948+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:46.949+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 17) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:47.014+0000] {subprocess.py:93} INFO - 25/08/18 13:52:46 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 18) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:47.049+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO TaskSetManager: Starting task 1.1 in stage 1.0 (TID 19) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:47.054+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 7) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:47.057+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.066+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.068+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T13:52:47.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.077+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.079+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.081+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.082+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.107+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.111+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.112+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.114+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.117+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 5) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:47.120+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.128+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-18T13:52:47.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.141+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.142+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.143+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.144+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.148+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.149+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.149+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.151+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.152+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.154+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.157+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.158+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.159+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 20) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:47.194+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 21) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:47.197+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO TaskSetManager: Starting task 1.1 in stage 3.0 (TID 22) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:47.242+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 6) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:47.243+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.258+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T13:52:47.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.292+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.300+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.308+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.309+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.309+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.310+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.317+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.317+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 23) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:47.317+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 14) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-df42eb8a-b141-493b-a7f1-97684c27ffff-c000.avro
[2025-08-18T13:52:47.323+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.332+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.336+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-08-18T13:52:47.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.363+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.376+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.377+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.382+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.384+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.385+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 15) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-df42eb8a-b141-493b-a7f1-97684c27ffff-c000.avro
[2025-08-18T13:52:47.385+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.401+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-08-18T13:52:47.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.422+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.450+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.452+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.458+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.459+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.459+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 13) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-aff13ade-ecac-4dd9-aa39-0a63b8d5f26d-c000.avro
[2025-08-18T13:52:47.460+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.469+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.471+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-08-18T13:52:47.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.497+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.527+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.527+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.529+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.530+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.533+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 12) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00005-aff13ade-ecac-4dd9-aa39-0a63b8d5f26d-c000.avro
[2025-08-18T13:52:47.535+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:47.540+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:47.541+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:47.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:47.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:47.547+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:47.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-08-18T13:52:47.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:47.552+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:47.554+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:47.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:47.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:47.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:47.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:47.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:47.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:47.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:47.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:47.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:47.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:47.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:47.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:47.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:47.580+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:47.583+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:47.586+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:47.589+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:47.590+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:47.592+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:47.595+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:47.797+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:47.939+0000] {subprocess.py:93} INFO - 25/08/18 13:52:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:48.024+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.1 in stage 6.0 (TID 24) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.035+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 9) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00011-51a0254a-ab03-4b3d-8c60-3fff7fd314a7-c000.avro
[2025-08-18T13:52:48.038+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:48.039+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:48.040+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:48.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:48.045+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:48.046+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:48.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
[2025-08-18T13:52:48.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:48.055+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:48.057+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:48.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:48.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:48.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:48.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:48.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:48.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:48.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:48.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:48.078+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:48.078+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:48.079+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:48.079+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:48.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:48.087+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:48.089+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:48.091+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:48.096+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:48.097+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.1 in stage 4.0 (TID 25) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.142+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.1 in stage 6.0 (TID 26) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.148+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.1 in stage 7.0 (TID 27) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.225+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 10) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-ae488739-b2e8-4be2-a218-0f28690e6317-c000.avro
[2025-08-18T13:52:48.226+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:48.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:48.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:48.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:48.234+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:48.237+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:48.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-18T13:52:48.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:48.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:48.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:48.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:48.252+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:48.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:48.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:48.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:48.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:48.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:48.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:48.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:48.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:48.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:48.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:48.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:48.286+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:48.290+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:48.294+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:48.295+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:48.296+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 8) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-51a0254a-ab03-4b3d-8c60-3fff7fd314a7-c000.avro
[2025-08-18T13:52:48.299+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:48.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:48.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:48.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:48.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:48.312+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:48.313+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
[2025-08-18T13:52:48.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:48.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:48.317+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:48.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:48.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:48.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:48.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:48.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:48.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:48.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:48.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:48.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:48.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:48.344+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:48.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:48.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:48.350+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:48.351+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:48.352+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:48.355+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:48.358+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.1 in stage 4.0 (TID 28) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.359+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.13:43991 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:48.360+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.13:43991 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:48.361+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:48.361+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 11) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ae488739-b2e8-4be2-a218-0f28690e6317-c000.avro
[2025-08-18T13:52:48.361+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:48.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:48.369+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:48.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:48.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:48.373+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:48.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-18T13:52:48.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:48.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:48.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:48.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:48.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:48.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:48.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:48.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:48.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:48.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:48.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:48.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:48.395+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:48.397+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:48.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:48.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:48.405+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:48.405+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:48.406+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:48.406+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:48.406+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.1 in stage 5.0 (TID 29) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.413+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 20) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:48.414+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.415+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 21) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:48.418+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.429+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.1 in stage 3.0 (TID 22) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:48.430+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.431+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 30) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.509+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 31) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.511+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.1 in stage 1.0 (TID 19) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00011-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:48.513+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.521+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.2 in stage 1.0 (TID 32) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.522+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 16) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:48.523+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.524+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 33) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.552+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.2 in stage 3.0 (TID 34) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.557+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 35) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.559+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.1 in stage 7.0 (TID 36) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.561+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 17) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:48.562+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.592+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 30) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:48.596+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:48.602+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 23) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:48.603+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.604+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.12:34115 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:48.605+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 18) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00005-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:48.606+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.637+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:48.673+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 37) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.674+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 33) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:48.676+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:48.679+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 38) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.683+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T13:52:48.694+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.2 in stage 3.0 (TID 34) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:48.694+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:48.769+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 39) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.771+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.1 in stage 4.0 (TID 25) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00011-51a0254a-ab03-4b3d-8c60-3fff7fd314a7-c000.avro
[2025-08-18T13:52:48.772+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:48.806+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.12:34115 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:48.852+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 40) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:48.856+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 37) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:48.857+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:48.890+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 41) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:48.894+0000] {subprocess.py:93} INFO - 25/08/18 13:52:48 INFO TaskSetManager: Starting task 1.3 in stage 3.0 (TID 42) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:49.011+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 38) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:49.012+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T13:52:49.016+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
[2025-08-18T13:52:49.038+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 43) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:49.139+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Starting task 1.2 in stage 4.0 (TID 44) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:49.143+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 45) (172.18.0.14, executor 0, partition 0, ANY, 9104 bytes)
[2025-08-18T13:52:49.144+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Lost task 0.1 in stage 7.0 (TID 36) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-df42eb8a-b141-493b-a7f1-97684c27ffff-c000.avro
[2025-08-18T13:52:49.145+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:49.218+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Starting task 0.2 in stage 7.0 (TID 46) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:49.220+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Lost task 1.3 in stage 3.0 (TID 42) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:49.221+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T13:52:49.243+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 ERROR TaskSetManager: Task 1 in stage 3.0 failed 4 times; aborting job
[2025-08-18T13:52:49.253+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Lost task 1.2 in stage 2.0 (TID 31) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:49.254+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:49.269+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:52:49.311+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.12:34115 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:49.340+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:52:49.352+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T13:52:49.364+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 47) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:49.371+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-08-18T13:52:49.376+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 38) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:49.376+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:49.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:49.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:49.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:49.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:49.381+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:49.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T13:52:49.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:49.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:49.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:49.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:49.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:49.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:49.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:49.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:49.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:49.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:49.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:49.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:49.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:49.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:49.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:49.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:49.418+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:49.425+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:49.426+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:49.427+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:49.428+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:49.453+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.12:34115 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:52:49.480+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:49.689+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-08-18T13:52:49.696+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Stage 0 was cancelled
[2025-08-18T13:52:49.700+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 177.986 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 38) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:49.701+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:49.701+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:49.702+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:49.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:49.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:49.706+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:49.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T13:52:49.716+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:49.717+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:49.718+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:49.718+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:49.719+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:49.721+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:49.722+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:49.722+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:49.722+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:49.723+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:49.723+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:49.723+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:49.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:49.726+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:49.727+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:49.728+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:49.729+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:49.729+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:49.730+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:49.731+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:49.732+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:49.751+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.12:34115 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:49.753+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.14:32985 (size: 34.7 KiB, free: 434.0 MiB)
[2025-08-18T13:52:49.757+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 40) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:49.758+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T13:52:49.758+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job
[2025-08-18T13:52:49.760+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Job 0 failed: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 178.317158 s
[2025-08-18T13:52:49.779+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Cancelling stage 3
[2025-08-18T13:52:49.779+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 42) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:49.780+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:49.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:49.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:49.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:49.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:49.787+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:49.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T13:52:49.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:49.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:49.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:49.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:49.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:49.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:49.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:49.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:49.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:49.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:49.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:49.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:49.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:49.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:49.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:49.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:49.800+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:49.800+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:49.801+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:49.801+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:49.801+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:49.876+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 5d9cfcc7-69f8-4497-8bac-3d582d6c3882)
[2025-08-18T13:52:49.917+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 14a216e3-b298-4e31-9138-949104635e9e)
[2025-08-18T13:52:49.922+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId b7bffe00-c5a1-47c1-82de-d8827cec7a12)
[2025-08-18T13:52:49.922+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 55108fd5-dff9-46af-a02b-c4da6fc8ac95)
[2025-08-18T13:52:49.924+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:49.934+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 17b80056-0e63-459b-a6b4-28b888677f69)
[2025-08-18T13:52:49.934+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 58548bcd-d4ea-4ab9-92f4-e2caba175c35)
[2025-08-18T13:52:49.935+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId ae83b56f-3526-4498-b9ce-f1a627c0f116)
[2025-08-18T13:52:49.937+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 14df8af5-0c11-4e64-9448-cc9b95eeb491)
[2025-08-18T13:52:49.937+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSchedulerImpl: Stage 3 was cancelled
[2025-08-18T13:52:49.938+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 177.729 s due to Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 42) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:49.938+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:49.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:49.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:49.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:49.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:49.941+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:49.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T13:52:49.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:49.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:49.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:49.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:49.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:49.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:49.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:49.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:49.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:49.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:49.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:49.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:49.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:49.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:49.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:49.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:49.955+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:49.955+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:49.956+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:49.956+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:49.956+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:49.957+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Lost task 1.2 in stage 1.0 (TID 32) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00011-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:49.957+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:49.970+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 71b32af7-c61f-4771-bd3a-7349e17a9a5f)
[2025-08-18T13:52:50.023+0000] {subprocess.py:93} INFO - 25/08/18 13:52:49 INFO TaskSetManager: Starting task 1.3 in stage 1.0 (TID 48) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:50.142+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 49) (172.18.0.14, executor 0, partition 1, ANY, 9104 bytes)
[2025-08-18T13:52:50.152+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 50) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:50.204+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:34115 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:50.235+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 51) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:50.261+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 1.3 in stage 0.0 (TID 41) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:50.265+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T13:52:50.268+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.278+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 35) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-ae488739-b2e8-4be2-a218-0f28690e6317-c000.avro
[2025-08-18T13:52:50.288+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:50.288+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 1.1 in stage 5.0 (TID 29) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ae488739-b2e8-4be2-a218-0f28690e6317-c000.avro
[2025-08-18T13:52:50.289+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:50.304+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 47) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:50.312+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T13:52:50.331+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.334+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 0.1 in stage 4.0 (TID 28) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-51a0254a-ab03-4b3d-8c60-3fff7fd314a7-c000.avro
[2025-08-18T13:52:50.338+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:50.367+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 2
[2025-08-18T13:52:50.411+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 40) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:50.414+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:50.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:50.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:50.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:50.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:50.425+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:50.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-18T13:52:50.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:50.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:50.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:50.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:50.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:50.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:50.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:50.458+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:50.459+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:50.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:50.462+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:50.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:50.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:50.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:50.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:50.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:50.484+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:50.486+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:50.489+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:50.493+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:50.497+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:50.499+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.258 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 40) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-c1670317-cb78-4a6c-a7b2-757e8e5171b9-c000.avro
[2025-08-18T13:52:50.504+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:50.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:50.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:50.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:50.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:50.525+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:50.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-18T13:52:50.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:50.529+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:50.533+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:50.536+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:50.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:50.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:50.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:50.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:50.549+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:50.549+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:50.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:50.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:50.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:50.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:50.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:50.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:50.564+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:50.570+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:50.571+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:50.571+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:50.579+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:50.580+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 43) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-08836d7c-2e39-4f33-8c16-9e23ecdacea8-c000.avro
[2025-08-18T13:52:50.586+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:50.589+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.590+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Starting task 0.2 in stage 4.0 (TID 52) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:50.591+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Starting task 1.2 in stage 5.0 (TID 53) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T13:52:50.595+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 54) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T13:52:50.596+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.13:43991 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:50.602+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.13:43991 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:50.603+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.14:32985 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:52:50.604+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 45) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00005-470fb0ee-0274-44de-a1cf-12379414688d-c000.avro
[2025-08-18T13:52:50.605+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:50.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:50.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:50.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:50.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:50.615+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:50.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
[2025-08-18T13:52:50.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:50.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:50.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:50.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:50.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:50.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:50.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:50.644+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:50.647+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:50.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:50.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:50.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:50.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:50.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:50.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:50.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:50.677+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:50.684+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:50.685+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:50.690+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:50.692+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 1.3 in stage 1.0 (TID 48) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00011-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:50.696+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T13:52:50.699+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 ERROR TaskSetManager: Task 1 in stage 1.0 failed 4 times; aborting job
[2025-08-18T13:52:50.702+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.708+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 0.1 in stage 6.0 (TID 24) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00005-aff13ade-ecac-4dd9-aa39-0a63b8d5f26d-c000.avro
[2025-08-18T13:52:50.709+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:50.713+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 6
[2025-08-18T13:52:50.715+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage cancelled: Job 6 cancelled part of cancelled job tag broadcast exchange (runId 5d9cfcc7-69f8-4497-8bac-3d582d6c3882)
[2025-08-18T13:52:50.719+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.721+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Stage 6 was cancelled
[2025-08-18T13:52:50.725+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.044 s due to Job 6 cancelled part of cancelled job tag broadcast exchange (runId 5d9cfcc7-69f8-4497-8bac-3d582d6c3882)
[2025-08-18T13:52:50.730+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 7
[2025-08-18T13:52:50.771+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage cancelled: Job 7 cancelled part of cancelled job tag broadcast exchange (runId 14a216e3-b298-4e31-9138-949104635e9e)
[2025-08-18T13:52:50.772+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Stage 7 was cancelled
[2025-08-18T13:52:50.776+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.088 s due to Job 7 cancelled part of cancelled job tag broadcast exchange (runId 14a216e3-b298-4e31-9138-949104635e9e)
[2025-08-18T13:52:50.779+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 1.1 in stage 6.0 (TID 26) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-aff13ade-ecac-4dd9-aa39-0a63b8d5f26d-c000.avro
[2025-08-18T13:52:50.786+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:50.787+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.791+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 1.1 in stage 7.0 (TID 27) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-df42eb8a-b141-493b-a7f1-97684c27ffff-c000.avro
[2025-08-18T13:52:50.793+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T13:52:50.801+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 4
[2025-08-18T13:52:50.802+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled: Job 4 cancelled part of cancelled job tag broadcast exchange (runId b7bffe00-c5a1-47c1-82de-d8827cec7a12)
[2025-08-18T13:52:50.804+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Stage 4 was cancelled
[2025-08-18T13:52:50.805+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.310 s due to Job 4 cancelled part of cancelled job tag broadcast exchange (runId b7bffe00-c5a1-47c1-82de-d8827cec7a12)
[2025-08-18T13:52:50.809+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 9
[2025-08-18T13:52:50.812+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId 17b80056-0e63-459b-a6b4-28b888677f69)
[2025-08-18T13:52:50.818+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Stage 9 was cancelled
[2025-08-18T13:52:50.820+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 39) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00005-e95c1eb4-d247-443d-bee2-1f8302e3bb3c-c000.avro
[2025-08-18T13:52:50.823+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:50.824+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.044 s due to Job 9 cancelled part of cancelled job tag broadcast exchange (runId 17b80056-0e63-459b-a6b4-28b888677f69)
[2025-08-18T13:52:50.824+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.825+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 8
[2025-08-18T13:52:50.825+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage cancelled: Job 8 cancelled part of cancelled job tag broadcast exchange (runId ae83b56f-3526-4498-b9ce-f1a627c0f116)
[2025-08-18T13:52:50.825+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Stage 8 was cancelled
[2025-08-18T13:52:50.833+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.091 s due to Job 8 cancelled part of cancelled job tag broadcast exchange (runId ae83b56f-3526-4498-b9ce-f1a627c0f116)
[2025-08-18T13:52:50.837+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 1
[2025-08-18T13:52:50.838+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job 1 cancelled part of cancelled job tag broadcast exchange (runId 14df8af5-0c11-4e64-9448-cc9b95eeb491)
[2025-08-18T13:52:50.839+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.760 s due to Job 1 cancelled part of cancelled job tag broadcast exchange (runId 14df8af5-0c11-4e64-9448-cc9b95eeb491)
[2025-08-18T13:52:50.926+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.13:43991 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T13:52:50.942+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 49) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00003-470fb0ee-0274-44de-a1cf-12379414688d-c000.avro
[2025-08-18T13:52:50.943+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:50.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:50.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:50.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:50.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:50.952+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:50.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
[2025-08-18T13:52:50.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:50.961+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:50.962+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:50.965+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:50.966+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:50.971+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:50.972+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:50.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:50.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:50.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:50.978+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:50.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:50.982+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:50.983+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:50.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:50.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:50.988+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:50.990+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:50.991+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:50.992+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:50.994+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-08-18T13:52:50.997+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 54) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-ae488739-b2e8-4be2-a218-0f28690e6317-c000.avro
[2025-08-18T13:52:50.998+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:51.000+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Cancelling stage 5
[2025-08-18T13:52:51.001+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job 5 cancelled part of cancelled job tag broadcast exchange (runId 71b32af7-c61f-4771-bd3a-7349e17a9a5f)
[2025-08-18T13:52:51.002+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO TaskSchedulerImpl: Stage 5 was cancelled
[2025-08-18T13:52:51.004+0000] {subprocess.py:93} INFO - 25/08/18 13:52:50 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 178.451 s due to Job 5 cancelled part of cancelled job tag broadcast exchange (runId 71b32af7-c61f-4771-bd3a-7349e17a9a5f)
[2025-08-18T13:52:51.028+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 50) (172.18.0.14 executor 0): TaskKilled (Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId 17b80056-0e63-459b-a6b4-28b888677f69))
[2025-08-18T13:52:51.029+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-08-18T13:52:51.036+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSetManager: Lost task 0.2 in stage 4.0 (TID 52) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-51a0254a-ab03-4b3d-8c60-3fff7fd314a7-c000.avro
[2025-08-18T13:52:51.038+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:51.038+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-08-18T13:52:51.042+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSetManager: Lost task 0.2 in stage 7.0 (TID 46) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-df42eb8a-b141-493b-a7f1-97684c27ffff-c000.avro
[2025-08-18T13:52:51.044+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:51.044+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-08-18T13:52:51.045+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 51) (172.18.0.14 executor 0): TaskKilled (Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId 17b80056-0e63-459b-a6b4-28b888677f69))
[2025-08-18T13:52:51.045+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-08-18T13:52:51.053+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSetManager: Lost task 1.2 in stage 4.0 (TID 44) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00011-51a0254a-ab03-4b3d-8c60-3fff7fd314a7-c000.avro
[2025-08-18T13:52:51.053+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T13:52:51.069+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-08-18T13:52:51.179+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 WARN TaskSetManager: Lost task 1.2 in stage 5.0 (TID 53) (172.18.0.13 executor 1): TaskKilled (Stage cancelled: Job 5 cancelled part of cancelled job tag broadcast exchange (runId 71b32af7-c61f-4771-bd3a-7349e17a9a5f))
[2025-08-18T13:52:51.221+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-08-18T13:52:51.257+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-08-18T13:52:51.259+0000] {subprocess.py:93} INFO -   File "/opt/***/jobs/transformation/transform_to_golden_dataset.py", line 135, in <module>
[2025-08-18T13:52:51.260+0000] {subprocess.py:93} INFO -     final_df.write.format("parquet").mode("overwrite").save("hdfs://namenode:9000/storage/hdfs/processed/golden_dataset")
[2025-08-18T13:52:51.261+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1463, in save
[2025-08-18T13:52:51.263+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-08-18T13:52:51.265+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-08-18T13:52:51.268+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-08-18T13:52:51.271+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o1969.save.
[2025-08-18T13:52:51.271+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 38) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:51.271+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:51.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:51.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:51.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:51.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:51.275+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:51.278+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T13:52:51.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:51.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:51.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:51.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:51.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:51.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:51.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:51.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:51.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:51.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:51.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:51.292+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:51.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:51.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:51.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:51.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:51.296+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:51.297+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:51.297+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:51.302+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:51.303+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T13:52:51.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-08-18T13:52:51.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-08-18T13:52:51.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-08-18T13:52:51.307+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-08-18T13:52:51.307+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-08-18T13:52:51.310+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-08-18T13:52:51.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-08-18T13:52:51.312+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-08-18T13:52:51.315+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-08-18T13:52:51.315+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-08-18T13:52:51.317+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-08-18T13:52:51.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-08-18T13:52:51.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-08-18T13:52:51.320+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-08-18T13:52:51.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-08-18T13:52:51.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-08-18T13:52:51.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-08-18T13:52:51.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-08-18T13:52:51.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-08-18T13:52:51.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
[2025-08-18T13:52:51.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
[2025-08-18T13:52:51.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-08-18T13:52:51.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2025-08-18T13:52:51.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
[2025-08-18T13:52:51.329+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
[2025-08-18T13:52:51.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)
[2025-08-18T13:52:51.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)
[2025-08-18T13:52:51.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)
[2025-08-18T13:52:51.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-18T13:52:51.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)
[2025-08-18T13:52:51.335+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2025-08-18T13:52:51.335+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T13:52:51.335+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T13:52:51.336+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T13:52:51.336+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-cf623687-a4f9-4dbc-99f4-7013d5e0d5c5-c000.avro
[2025-08-18T13:52:51.337+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T13:52:51.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T13:52:51.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T13:52:51.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T13:52:51.338+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T13:52:51.338+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T13:52:51.338+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T13:52:51.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T13:52:51.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T13:52:51.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T13:52:51.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T13:52:51.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T13:52:51.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T13:52:51.341+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T13:52:51.341+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T13:52:51.341+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T13:52:51.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T13:52:51.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T13:52:51.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T13:52:51.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T13:52:51.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T13:52:51.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T13:52:51.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T13:52:51.344+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-08-18T13:52:51.344+0000] {subprocess.py:93} INFO - 
[2025-08-18T13:52:51.478+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-18T13:52:51.491+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-08-18T13:52:51.605+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO SparkUI: Stopped Spark web UI at http://4bf6b336ccb3:4040
[2025-08-18T13:52:51.633+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-18T13:52:51.643+0000] {subprocess.py:93} INFO - 25/08/18 13:52:51 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-18T13:52:52.065+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-18T13:52:52.442+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO MemoryStore: MemoryStore cleared
[2025-08-18T13:52:52.450+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO BlockManager: BlockManager stopped
[2025-08-18T13:52:52.494+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-18T13:52:52.509+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-18T13:52:52.786+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO SparkContext: Successfully stopped SparkContext
[2025-08-18T13:52:52.787+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO ShutdownHookManager: Shutdown hook called
[2025-08-18T13:52:52.895+0000] {subprocess.py:93} INFO - 25/08/18 13:52:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-abd46729-5ba9-4fc9-a9e3-1156733c9af4
[2025-08-18T13:52:53.110+0000] {subprocess.py:93} INFO - 25/08/18 13:52:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-5ed76fa8-f456-4c11-ae85-0447d39be9f6
[2025-08-18T13:52:53.179+0000] {subprocess.py:93} INFO - 25/08/18 13:52:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-abd46729-5ba9-4fc9-a9e3-1156733c9af4/pyspark-da9fc150-1f12-42c7-a4cd-b7c4b7c38c01
[2025-08-18T13:52:53.818+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-08-18T13:52:53.822+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-08-18T13:52:53.871+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-08-18T13:52:53.886+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=youtube_pipeline_batch_data, task_id=transform_golden, execution_date=20250818T134453, start_date=20250818T134852, end_date=20250818T135253
[2025-08-18T13:52:53.935+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 349 for task transform_golden (Bash command failed. The command returned a non-zero exit code 1.; 5454)
[2025-08-18T13:52:53.994+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-08-18T13:52:54.112+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-18T13:52:54.121+0000] {local_task_job_runner.py:222} INFO - ::endgroup::

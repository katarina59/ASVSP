[2025-08-18T13:52:32.250+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-08-18T13:52:32.449+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-18T13:45:21.919513+00:00 [queued]>
[2025-08-18T13:52:32.509+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-18T13:45:21.919513+00:00 [queued]>
[2025-08-18T13:52:32.511+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 4
[2025-08-18T13:52:32.603+0000] {taskinstance.py:2327} INFO - Executing <Task(BashOperator): transform_golden> on 2025-08-18 13:45:21.919513+00:00
[2025-08-18T13:52:32.627+0000] {standard_task_runner.py:63} INFO - Started process 5597 to run task
[2025-08-18T13:52:32.627+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'youtube_pipeline_batch_data', 'transform_golden', 'manual__2025-08-18T13:45:21.919513+00:00', '--job-id', '358', '--raw', '--subdir', 'DAGS_FOLDER/youtube_pipeline.py', '--cfg-path', '/tmp/tmpyl0eg1eo']
[2025-08-18T13:52:32.645+0000] {standard_task_runner.py:91} INFO - Job 358: Subtask transform_golden
[2025-08-18T13:52:32.841+0000] {task_command.py:426} INFO - Running <TaskInstance: youtube_pipeline_batch_data.transform_golden manual__2025-08-18T13:45:21.919513+00:00 [running]> on host 47205959681c
[2025-08-18T13:52:33.254+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='youtube_pipeline_batch_data' AIRFLOW_CTX_TASK_ID='transform_golden' AIRFLOW_CTX_EXECUTION_DATE='2025-08-18T13:45:21.919513+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-18T13:45:21.919513+00:00'
[2025-08-18T13:52:33.256+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-08-18T13:52:33.311+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-08-18T13:52:33.315+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'docker exec youtube-pipeline-project_spark-master_1 spark-submit --master spark://spark-master:7077 --deploy-mode client --packages org.apache.spark:spark-avro_2.12:3.5.1 /opt/***/jobs/transformation/transform_to_golden_dataset.py']
[2025-08-18T13:52:33.374+0000] {subprocess.py:86} INFO - Output:
[2025-08-18T13:52:52.344+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-08-18T13:52:53.140+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2025-08-18T13:52:53.141+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-08-18T13:52:53.144+0000] {subprocess.py:93} INFO - org.apache.spark#spark-avro_2.12 added as a dependency
[2025-08-18T13:52:53.145+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-358b6e34-19e9-4361-8cd7-11013d36e773;1.0
[2025-08-18T13:52:53.145+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-08-18T13:52:55.203+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-avro_2.12;3.5.1 in central
[2025-08-18T13:52:55.300+0000] {subprocess.py:93} INFO - 	found org.tukaani#xz;1.9 in central
[2025-08-18T13:52:55.341+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 2179ms :: artifacts dl 19ms
[2025-08-18T13:52:55.343+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-08-18T13:52:55.345+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-avro_2.12;3.5.1 from central in [default]
[2025-08-18T13:52:55.350+0000] {subprocess.py:93} INFO - 	org.tukaani#xz;1.9 from central in [default]
[2025-08-18T13:52:55.352+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-18T13:52:55.353+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-08-18T13:52:55.354+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-08-18T13:52:55.355+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-18T13:52:55.356+0000] {subprocess.py:93} INFO - 	|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
[2025-08-18T13:52:55.360+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-08-18T13:52:55.361+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-358b6e34-19e9-4361-8cd7-11013d36e773
[2025-08-18T13:52:55.361+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-08-18T13:52:55.382+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 2 already retrieved (0kB/25ms)
[2025-08-18T13:52:57.636+0000] {subprocess.py:93} INFO - 25/08/18 13:52:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-18T13:53:03.826+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO SparkContext: Running Spark version 3.5.1
[2025-08-18T13:53:03.828+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO SparkContext: OS info Linux, 6.11.0-29-generic, amd64
[2025-08-18T13:53:03.834+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO SparkContext: Java version 17.0.12
[2025-08-18T13:53:03.955+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO ResourceUtils: ==============================================================
[2025-08-18T13:53:03.957+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-18T13:53:03.959+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO ResourceUtils: ==============================================================
[2025-08-18T13:53:03.964+0000] {subprocess.py:93} INFO - 25/08/18 13:53:03 INFO SparkContext: Submitted application: Processed Zone - Golden Dataset
[2025-08-18T13:53:04.062+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-18T13:53:04.100+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO ResourceProfile: Limiting resource is cpu
[2025-08-18T13:53:04.102+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-18T13:53:04.286+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO SecurityManager: Changing view acls to: spark
[2025-08-18T13:53:04.289+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO SecurityManager: Changing modify acls to: spark
[2025-08-18T13:53:04.300+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO SecurityManager: Changing view acls groups to:
[2025-08-18T13:53:04.305+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO SecurityManager: Changing modify acls groups to:
[2025-08-18T13:53:04.308+0000] {subprocess.py:93} INFO - 25/08/18 13:53:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-08-18T13:53:07.383+0000] {subprocess.py:93} INFO - 25/08/18 13:53:07 INFO Utils: Successfully started service 'sparkDriver' on port 39811.
[2025-08-18T13:53:07.576+0000] {subprocess.py:93} INFO - 25/08/18 13:53:07 INFO SparkEnv: Registering MapOutputTracker
[2025-08-18T13:53:08.001+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-18T13:53:08.073+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-18T13:53:08.078+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-18T13:53:08.111+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-18T13:53:08.228+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1dd37741-707e-4cbb-8863-e5541b75455b
[2025-08-18T13:53:08.328+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-18T13:53:08.392+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-18T13:53:08.747+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-18T13:53:08.911+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-08-18T13:53:08.911+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2025-08-18T13:53:08.912+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2025-08-18T13:53:08.930+0000] {subprocess.py:93} INFO - 25/08/18 13:53:08 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2025-08-18T13:53:09.414+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar at spark://4bf6b336ccb3:39811/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar with timestamp 1755525183806
[2025-08-18T13:53:09.415+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://4bf6b336ccb3:39811/jars/org.tukaani_xz-1.9.jar with timestamp 1755525183806
[2025-08-18T13:53:09.421+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar at spark://4bf6b336ccb3:39811/files/org.apache.spark_spark-avro_2.12-3.5.1.jar with timestamp 1755525183806
[2025-08-18T13:53:09.423+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.1.jar to /tmp/spark-ca094dea-b92a-4af1-9e2b-292ae7375e47/userFiles-dcc8e00c-5dde-49cb-b4f9-1430df300fcd/org.apache.spark_spark-avro_2.12-3.5.1.jar
[2025-08-18T13:53:09.440+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar at spark://4bf6b336ccb3:39811/files/org.tukaani_xz-1.9.jar with timestamp 1755525183806
[2025-08-18T13:53:09.441+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.tukaani_xz-1.9.jar to /tmp/spark-ca094dea-b92a-4af1-9e2b-292ae7375e47/userFiles-dcc8e00c-5dde-49cb-b4f9-1430df300fcd/org.tukaani_xz-1.9.jar
[2025-08-18T13:53:09.874+0000] {subprocess.py:93} INFO - 25/08/18 13:53:09 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-18T13:53:10.395+0000] {subprocess.py:93} INFO - 25/08/18 13:53:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.7:7077 after 261 ms (0 ms spent in bootstraps)
[2025-08-18T13:53:11.197+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250818135311-0047
[2025-08-18T13:53:11.244+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46689.
[2025-08-18T13:53:11.250+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO NettyBlockTransferService: Server created on 4bf6b336ccb3:46689
[2025-08-18T13:53:11.273+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-18T13:53:11.383+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4bf6b336ccb3, 46689, None)
[2025-08-18T13:53:11.429+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO BlockManagerMasterEndpoint: Registering block manager 4bf6b336ccb3:46689 with 434.4 MiB RAM, BlockManagerId(driver, 4bf6b336ccb3, 46689, None)
[2025-08-18T13:53:11.465+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4bf6b336ccb3, 46689, None)
[2025-08-18T13:53:11.486+0000] {subprocess.py:93} INFO - 25/08/18 13:53:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4bf6b336ccb3, 46689, None)
[2025-08-18T13:53:12.667+0000] {subprocess.py:93} INFO - 25/08/18 13:53:12 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-18T13:53:14.235+0000] {subprocess.py:93} INFO - 25/08/18 13:53:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-18T13:53:14.241+0000] {subprocess.py:93} INFO - 25/08/18 13:53:14 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2025-08-18T13:53:25.035+0000] {subprocess.py:93} INFO - 25/08/18 13:53:25 INFO InMemoryFileIndex: It took 668 ms to list leaf files for 1 paths.
[2025-08-18T13:53:31.989+0000] {subprocess.py:93} INFO - 25/08/18 13:53:31 INFO InMemoryFileIndex: It took 103 ms to list leaf files for 1 paths.
[2025-08-18T13:53:39.808+0000] {subprocess.py:93} INFO - 25/08/18 13:53:39 INFO InMemoryFileIndex: It took 79 ms to list leaf files for 1 paths.
[2025-08-18T13:53:39.974+0000] {subprocess.py:93} INFO - 25/08/18 13:53:39 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2025-08-18T13:53:43.132+0000] {subprocess.py:93} INFO - 25/08/18 13:53:43 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
[2025-08-18T13:53:43.568+0000] {subprocess.py:93} INFO - 25/08/18 13:53:43 INFO InMemoryFileIndex: It took 144 ms to list leaf files for 1 paths.
[2025-08-18T13:53:48.983+0000] {subprocess.py:93} INFO - 25/08/18 13:53:48 INFO InMemoryFileIndex: It took 156 ms to list leaf files for 1 paths.
[2025-08-18T13:53:49.534+0000] {subprocess.py:93} INFO - 25/08/18 13:53:49 INFO InMemoryFileIndex: It took 117 ms to list leaf files for 1 paths.
[2025-08-18T13:53:55.559+0000] {subprocess.py:93} INFO - 25/08/18 13:53:55 INFO InMemoryFileIndex: It took 83 ms to list leaf files for 1 paths.
[2025-08-18T13:53:55.879+0000] {subprocess.py:93} INFO - 25/08/18 13:53:55 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2025-08-18T13:53:59.852+0000] {subprocess.py:93} INFO - 25/08/18 13:53:59 INFO InMemoryFileIndex: It took 107 ms to list leaf files for 1 paths.
[2025-08-18T13:54:00.341+0000] {subprocess.py:93} INFO - 25/08/18 13:54:00 INFO InMemoryFileIndex: It took 87 ms to list leaf files for 1 paths.
[2025-08-18T13:54:06.986+0000] {subprocess.py:93} INFO - 25/08/18 13:54:06 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
[2025-08-18T13:54:07.231+0000] {subprocess.py:93} INFO - 25/08/18 13:54:07 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
[2025-08-18T13:54:09.509+0000] {subprocess.py:93} INFO - 25/08/18 13:54:09 INFO InMemoryFileIndex: It took 106 ms to list leaf files for 1 paths.
[2025-08-18T13:54:09.832+0000] {subprocess.py:93} INFO - 25/08/18 13:54:09 INFO InMemoryFileIndex: It took 89 ms to list leaf files for 1 paths.
[2025-08-18T13:54:14.417+0000] {subprocess.py:93} INFO - 25/08/18 13:54:14 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2025-08-18T13:54:14.661+0000] {subprocess.py:93} INFO - 25/08/18 13:54:14 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
[2025-08-18T13:54:20.591+0000] {subprocess.py:93} INFO - 25/08/18 13:54:20 INFO InMemoryFileIndex: It took 81 ms to list leaf files for 1 paths.
[2025-08-18T13:54:20.994+0000] {subprocess.py:93} INFO - 25/08/18 13:54:20 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2025-08-18T13:54:49.076+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:49.132+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#7),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2, [^\x20-\x7E], , 1), None) END),(cast(views#7 as bigint) > 0)
[2025-08-18T13:54:49.379+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:49.380+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#32)
[2025-08-18T13:54:49.501+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:49.515+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#500),isnotnull(CASE WHEN (length(trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#495, [^\x20-\x7E], , 1), None) END),(cast(views#500 as bigint) > 0)
[2025-08-18T13:54:49.515+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:49.515+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#525)
[2025-08-18T13:54:49.520+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:49.521+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#993),isnotnull(CASE WHEN (length(trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#988, [^\x20-\x7E], , 1), None) END),(cast(views#993 as bigint) > 0)
[2025-08-18T13:54:49.522+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:49.523+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#1018)
[2025-08-18T13:54:49.633+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:49.635+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#1486),isnotnull(CASE WHEN (length(trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#1481, [^\x20-\x7E], , 1), None) END),(cast(views#1486 as bigint) > 0)
[2025-08-18T13:54:49.636+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:49.656+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#1511)
[2025-08-18T13:54:49.882+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:49.894+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#1979),isnotnull(CASE WHEN (length(trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#1974, [^\x20-\x7E], , 1), None) END),(cast(views#1979 as bigint) > 0)
[2025-08-18T13:54:49.903+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:49.916+0000] {subprocess.py:93} INFO - 25/08/18 13:54:49 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2004)
[2025-08-18T13:54:50.175+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:50.180+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#2472),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2467, [^\x20-\x7E], , 1), None) END),(cast(views#2472 as bigint) > 0)
[2025-08-18T13:54:50.226+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:50.226+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2497)
[2025-08-18T13:54:50.345+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:50.346+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#2965),isnotnull(CASE WHEN (length(trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#2960, [^\x20-\x7E], , 1), None) END),(cast(views#2965 as bigint) > 0)
[2025-08-18T13:54:50.346+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:50.347+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#2990)
[2025-08-18T13:54:50.362+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:50.363+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#3458),isnotnull(CASE WHEN (length(trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#3453, [^\x20-\x7E], , 1), None) END),(cast(views#3458 as bigint) > 0)
[2025-08-18T13:54:50.396+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:50.396+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#3483)
[2025-08-18T13:54:50.396+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:50.417+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#3951),isnotnull(CASE WHEN (length(trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#3946, [^\x20-\x7E], , 1), None) END),(cast(views#3951 as bigint) > 0)
[2025-08-18T13:54:50.418+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:50.420+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#3976)
[2025-08-18T13:54:50.424+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(views)
[2025-08-18T13:54:50.442+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(views#4444),isnotnull(CASE WHEN (length(trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None)) > 100) THEN concat(substring(trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None), 1, 97), ...) ELSE trim(regexp_replace(title#4439, [^\x20-\x7E], , 1), None) END),(cast(views#4444 as bigint) > 0)
[2025-08-18T13:54:50.443+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(category_id)
[2025-08-18T13:54:50.445+0000] {subprocess.py:93} INFO - 25/08/18 13:54:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(category_id#4469)
[2025-08-18T13:54:52.625+0000] {subprocess.py:93} INFO - 25/08/18 13:54:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-08-18T13:55:05.454+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 4791.56882 ms
[2025-08-18T13:55:05.466+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 4795.89851 ms
[2025-08-18T13:55:05.557+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 4847.917707 ms
[2025-08-18T13:55:05.578+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 4862.006702 ms
[2025-08-18T13:55:05.578+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 4917.102102 ms
[2025-08-18T13:55:05.784+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 4830.34584 ms
[2025-08-18T13:55:05.842+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 5171.123302 ms
[2025-08-18T13:55:05.843+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 5178.689654 ms
[2025-08-18T13:55:05.860+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 5120.01976 ms
[2025-08-18T13:55:05.900+0000] {subprocess.py:93} INFO - 25/08/18 13:55:05 INFO CodeGenerator: Code generated in 5241.584007 ms
[2025-08-18T13:55:06.505+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.9 KiB, free 434.2 MiB)
[2025-08-18T13:55:06.509+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.9 KiB, free 434.0 MiB)
[2025-08-18T13:55:06.514+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.9 KiB, free 433.8 MiB)
[2025-08-18T13:55:06.523+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.9 KiB, free 433.6 MiB)
[2025-08-18T13:55:06.622+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.9 KiB, free 433.4 MiB)
[2025-08-18T13:55:06.628+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 200.9 KiB, free 433.0 MiB)
[2025-08-18T13:55:06.631+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.9 KiB, free 433.0 MiB)
[2025-08-18T13:55:06.635+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.9 KiB, free 432.8 MiB)
[2025-08-18T13:55:06.652+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.9 KiB, free 432.6 MiB)
[2025-08-18T13:55:06.660+0000] {subprocess.py:93} INFO - 25/08/18 13:55:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.9 KiB, free 432.4 MiB)
[2025-08-18T13:55:07.967+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.4 MiB)
[2025-08-18T13:55:07.975+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.4 MiB)
[2025-08-18T13:55:07.981+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-18T13:55:07.987+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-18T13:55:07.995+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-18T13:55:08.001+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.2 MiB)
[2025-08-18T13:55:08.008+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T13:55:08.026+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-18T13:55:08.027+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-18T13:55:08.047+0000] {subprocess.py:93} INFO - 25/08/18 13:55:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.3 MiB)
[2025-08-18T13:55:08.052+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.1 MiB)
[2025-08-18T13:55:08.094+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:55:08.108+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:55:08.133+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T13:55:08.141+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.158+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.176+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.184+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 1 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.185+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:55:08.192+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:55:08.194+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.199+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.205+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T13:55:08.223+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.226+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:55:08.230+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:55:08.231+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4bf6b336ccb3:46689 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T13:55:08.237+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.251+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.258+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:08.390+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.395+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.411+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.414+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.415+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.417+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.429+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.429+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167907 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.430+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:08.430+0000] {subprocess.py:93} INFO - 25/08/18 13:55:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25167895 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-18T13:55:09.552+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.622+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.643+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.655+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.693+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.751+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.777+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.789+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.802+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.835+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-08-18T13:55:09.909+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:09.915+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:09.917+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:09.925+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:09.944+0000] {subprocess.py:93} INFO - 25/08/18 13:55:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.187+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 14.1 KiB, free 432.1 MiB)
[2025-08-18T13:55:10.191+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.1 MiB)
[2025-08-18T13:55:10.200+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T13:55:10.203+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.263+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.270+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.347+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.349+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.352+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.355+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.357+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.372+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.1 KiB, free 432.1 MiB)
[2025-08-18T13:55:10.375+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.1 MiB)
[2025-08-18T13:55:10.379+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.381+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.386+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.387+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.393+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.395+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.396+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.398+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.406+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.423+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.427+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.433+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.436+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.441+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.443+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.451+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.456+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.458+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.459+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.461+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.477+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.490+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.494+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.500+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.501+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.502+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.512+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.514+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.515+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.517+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.522+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.540+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.570+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.576+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.580+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.583+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[37] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.586+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.588+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.592+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.597+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.598+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.599+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.610+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.617+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.625+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.628+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.629+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.629+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.640+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.640+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.642+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.642+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.648+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.656+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.1 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.671+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 432.0 MiB)
[2025-08-18T13:55:10.671+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.672+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.675+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.680+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.680+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.681+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.691+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.691+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.692+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.692+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-18T13:55:10.739+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-18T13:55:10.739+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.740+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.740+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.746+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.753+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.755+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.758+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.759+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.764+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.769+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-18T13:55:10.782+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-18T13:55:10.782+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.809+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.810+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.810+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0
[2025-08-18T13:55:10.810+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2025-08-18T13:55:10.811+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-08-18T13:55:10.811+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-18T13:55:10.812+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Missing parents: List()
[2025-08-18T13:55:10.812+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-08-18T13:55:10.817+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.1 KiB, free 431.9 MiB)
[2025-08-18T13:55:10.839+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 431.9 MiB)
[2025-08-18T13:55:10.841+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 4bf6b336ccb3:46689 (size: 6.7 KiB, free: 434.0 MiB)
[2025-08-18T13:55:10.844+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2025-08-18T13:55:10.847+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[33] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2025-08-18T13:55:10.848+0000] {subprocess.py:93} INFO - 25/08/18 13:55:10 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0
[2025-08-18T13:55:25.349+0000] {subprocess.py:93} INFO - 25/08/18 13:55:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:55:40.342+0000] {subprocess.py:93} INFO - 25/08/18 13:55:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:55:55.343+0000] {subprocess.py:93} INFO - 25/08/18 13:55:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:56:10.343+0000] {subprocess.py:93} INFO - 25/08/18 13:56:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:56:25.343+0000] {subprocess.py:93} INFO - 25/08/18 13:56:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:56:40.353+0000] {subprocess.py:93} INFO - 25/08/18 13:56:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:56:55.350+0000] {subprocess.py:93} INFO - 25/08/18 13:56:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:57:10.343+0000] {subprocess.py:93} INFO - 25/08/18 13:57:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:57:25.345+0000] {subprocess.py:93} INFO - 25/08/18 13:57:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:57:40.350+0000] {subprocess.py:93} INFO - 25/08/18 13:57:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:57:55.343+0000] {subprocess.py:93} INFO - 25/08/18 13:57:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:58:10.357+0000] {subprocess.py:93} INFO - 25/08/18 13:58:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:58:25.344+0000] {subprocess.py:93} INFO - 25/08/18 13:58:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:58:40.343+0000] {subprocess.py:93} INFO - 25/08/18 13:58:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:58:55.346+0000] {subprocess.py:93} INFO - 25/08/18 13:58:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:59:10.349+0000] {subprocess.py:93} INFO - 25/08/18 13:59:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:59:25.351+0000] {subprocess.py:93} INFO - 25/08/18 13:59:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:59:40.345+0000] {subprocess.py:93} INFO - 25/08/18 13:59:40 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T13:59:55.344+0000] {subprocess.py:93} INFO - 25/08/18 13:59:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T14:00:08.994+0000] {subprocess.py:93} INFO - 25/08/18 14:00:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250818135311-0047/0 on worker-20250818100309-172.18.0.14-34047 (172.18.0.14:34047) with 4 core(s)
[2025-08-18T14:00:09.061+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250818135311-0047/0 on hostPort 172.18.0.14:34047 with 4 core(s), 1024.0 MiB RAM
[2025-08-18T14:00:09.081+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250818135311-0047/1 on worker-20250818100311-172.18.0.13-33759 (172.18.0.13:33759) with 4 core(s)
[2025-08-18T14:00:09.093+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250818135311-0047/1 on hostPort 172.18.0.13:33759 with 4 core(s), 1024.0 MiB RAM
[2025-08-18T14:00:09.099+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250818135311-0047/2 on worker-20250818100314-172.18.0.12-34745 (172.18.0.12:34745) with 4 core(s)
[2025-08-18T14:00:09.100+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250818135311-0047/2 on hostPort 172.18.0.12:34745 with 4 core(s), 1024.0 MiB RAM
[2025-08-18T14:00:09.855+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250818135311-0047/1 is now RUNNING
[2025-08-18T14:00:09.872+0000] {subprocess.py:93} INFO - 25/08/18 14:00:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250818135311-0047/2 is now RUNNING
[2025-08-18T14:00:10.013+0000] {subprocess.py:93} INFO - 25/08/18 14:00:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250818135311-0047/0 is now RUNNING
[2025-08-18T14:00:10.369+0000] {subprocess.py:93} INFO - 25/08/18 14:00:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T14:00:25.345+0000] {subprocess.py:93} INFO - 25/08/18 14:00:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2025-08-18T14:00:32.570+0000] {subprocess.py:93} INFO - 25/08/18 14:00:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:39092) with ID 2,  ResourceProfileId 0
[2025-08-18T14:00:32.608+0000] {subprocess.py:93} INFO - 25/08/18 14:00:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.14:32916) with ID 0,  ResourceProfileId 0
[2025-08-18T14:00:33.150+0000] {subprocess.py:93} INFO - 25/08/18 14:00:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:44081 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.12, 44081, None)
[2025-08-18T14:00:33.202+0000] {subprocess.py:93} INFO - 25/08/18 14:00:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.14:36279 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.14, 36279, None)
[2025-08-18T14:00:33.508+0000] {subprocess.py:93} INFO - 25/08/18 14:00:33 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.13:39328) with ID 1,  ResourceProfileId 0
[2025-08-18T14:00:34.081+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.13:42473 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.13, 42473, None)
[2025-08-18T14:00:34.408+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:34.534+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:34.542+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:34.544+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:34.641+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 4) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:34.652+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 5) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:34.665+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:34.674+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:34.817+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:34.819+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 9) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:34.871+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:34.880+0000] {subprocess.py:93} INFO - 25/08/18 14:00:34 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:36.942+0000] {subprocess.py:93} INFO - 25/08/18 14:00:36 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:36.967+0000] {subprocess.py:93} INFO - 25/08/18 14:00:36 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:36.977+0000] {subprocess.py:93} INFO - 25/08/18 14:00:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.12:44081 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:36.983+0000] {subprocess.py:93} INFO - 25/08/18 14:00:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.12:44081 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:37.438+0000] {subprocess.py:93} INFO - 25/08/18 14:00:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:37.486+0000] {subprocess.py:93} INFO - 25/08/18 14:00:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:43.481+0000] {subprocess.py:93} INFO - 25/08/18 14:00:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:43.515+0000] {subprocess.py:93} INFO - 25/08/18 14:00:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:43.601+0000] {subprocess.py:93} INFO - 25/08/18 14:00:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.12:44081 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:43.610+0000] {subprocess.py:93} INFO - 25/08/18 14:00:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.12:44081 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:44.455+0000] {subprocess.py:93} INFO - 25/08/18 14:00:44 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.4 MiB)
[2025-08-18T14:00:44.534+0000] {subprocess.py:93} INFO - 25/08/18 14:00:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:50.110+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:50.142+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:50.199+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:50.203+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:50.203+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.214+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.217+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.224+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-08-18T14:00:50.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.240+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.243+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.251+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.287+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.293+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.294+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.294+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.295+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 1) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:50.295+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.306+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T14:00:50.310+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.310+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.313+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.315+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.321+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.329+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.330+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.335+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.336+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.337+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.337+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.337+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 0) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:50.341+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.342+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.344+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.350+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T14:00:50.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.366+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.383+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.384+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.385+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.386+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.387+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 15) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:50.388+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 3) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:50.389+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.393+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.395+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
[2025-08-18T14:00:50.395+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.396+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.397+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.398+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.410+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.411+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.412+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.413+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.488+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:50.528+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:50.623+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 1.1 in stage 1.0 (TID 16) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:50.658+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 17) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:50.680+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 18) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:50.685+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 19) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:50.722+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 11) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:50.723+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.724+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.726+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.727+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.728+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.729+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T14:00:50.732+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.733+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.734+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.736+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.738+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.740+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.741+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.742+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.743+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.744+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.745+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.746+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.746+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.747+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.747+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.747+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.747+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.750+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.751+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.752+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.753+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 10) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:50.755+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.756+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.757+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.759+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.760+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.761+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.762+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)
[2025-08-18T14:00:50.763+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.765+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.770+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.771+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.772+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.773+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.774+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.776+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.777+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.778+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.778+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.780+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.780+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.780+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.790+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.790+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 15) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:50.791+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:50.791+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 8) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:50.792+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.797+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-18T14:00:50.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.803+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.804+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.805+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.806+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.809+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.811+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.811+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.812+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.813+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.813+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 20) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:50.813+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 9) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:50.815+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:50.816+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:50.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:50.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:50.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:50.819+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:50.820+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-18T14:00:50.821+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:50.821+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:50.821+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:50.822+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:50.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:50.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:50.825+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:50.825+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:50.826+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:50.826+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:50.826+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:50.827+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:50.828+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:50.829+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:50.830+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:50.831+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:50.831+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:50.832+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:50.833+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:50.834+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:50.985+0000] {subprocess.py:93} INFO - 25/08/18 14:00:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:51.152+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:51.246+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:51.399+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:51.436+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO TaskSetManager: Starting task 1.1 in stage 3.0 (TID 21) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:51.462+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 22) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:51.471+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 13) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-11231ceb-99cd-4f85-8916-e84173e105b6-c000.avro
[2025-08-18T14:00:51.472+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:51.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:51.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:51.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:51.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:51.474+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:51.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-18T14:00:51.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:51.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:51.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:51.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:51.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:51.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:51.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:51.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:51.494+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:51.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:51.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:51.500+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:51.501+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:51.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:51.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:51.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:51.505+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:51.510+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:51.512+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:51.516+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:51.517+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 12) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-11231ceb-99cd-4f85-8916-e84173e105b6-c000.avro
[2025-08-18T14:00:51.521+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:51.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:51.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:51.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:51.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:51.527+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:51.528+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
[2025-08-18T14:00:51.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:51.541+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:51.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:51.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:51.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:51.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:51.548+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:51.556+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:51.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:51.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:51.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:51.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:51.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:51.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:51.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:51.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:51.573+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:51.573+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:51.574+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:51.581+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:51.602+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 23) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:51.611+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO TaskSetManager: Starting task 1.1 in stage 5.0 (TID 24) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:51.619+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 14) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00005-12fc1c68-c95d-4a9e-bb2e-2e537a52ff9f-c000.avro
[2025-08-18T14:00:51.620+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:51.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:51.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:51.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:51.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:51.638+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:51.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-08-18T14:00:51.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:51.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:51.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:51.647+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:51.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:51.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:51.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:51.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:51.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:51.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:51.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:51.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:51.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:51.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:51.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:51.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:51.669+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:51.669+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:51.669+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:51.676+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:51.678+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 19) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-12fc1c68-c95d-4a9e-bb2e-2e537a52ff9f-c000.avro
[2025-08-18T14:00:51.681+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:51.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:51.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:51.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:51.691+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:51.691+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:51.692+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-08-18T14:00:51.692+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:51.692+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:51.693+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:51.697+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:51.697+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:51.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:51.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:51.702+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:51.702+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:51.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:51.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:51.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:51.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:51.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:51.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:51.713+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:51.715+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:51.717+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:51.717+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:51.718+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:51.718+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:51.823+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:51.883+0000] {subprocess.py:93} INFO - 25/08/18 14:00:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.024+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.029+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.099+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.1 in stage 6.0 (TID 25) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.100+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.1 in stage 6.0 (TID 26) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.135+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.1 in stage 7.0 (TID 27) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.142+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.1 in stage 7.0 (TID 28) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.151+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 0.1 in stage 2.0 (TID 18) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:52.154+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.155+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 20) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:52.156+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:52.157+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 1.1 in stage 1.0 (TID 16) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:52.162+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.164+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 17) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:52.164+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.320+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.2 in stage 1.0 (TID 29) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.357+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 1.1 in stage 3.0 (TID 21) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:52.358+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.364+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 30) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.387+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 31) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.397+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.399+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.410+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 32) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.412+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.2 in stage 3.0 (TID 33) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.418+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 34) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.419+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 7) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00011-8de414d7-5fe0-476f-80fa-8b132004bcf7-c000.avro
[2025-08-18T14:00:52.420+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:52.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:52.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:52.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:52.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:52.427+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:52.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)
[2025-08-18T14:00:52.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:52.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:52.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:52.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:52.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:52.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:52.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:52.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:52.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:52.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:52.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:52.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:52.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:52.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:52.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:52.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:52.456+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:52.464+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:52.464+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:52.465+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:52.465+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 4) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00005-662e3d18-5c46-4180-8ba6-a16ede2f2a43-c000.avro
[2025-08-18T14:00:52.465+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:52.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:52.469+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:52.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:52.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:52.474+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:52.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
[2025-08-18T14:00:52.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:52.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:52.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:52.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:52.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:52.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:52.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:52.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:52.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:52.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:52.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:52.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:52.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:52.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:52.495+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:52.495+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:52.496+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:52.496+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:52.496+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:52.497+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:52.501+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 5) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00011-662e3d18-5c46-4180-8ba6-a16ede2f2a43-c000.avro
[2025-08-18T14:00:52.505+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:52.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:52.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:52.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:52.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:52.510+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:52.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
[2025-08-18T14:00:52.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:52.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:52.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:52.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:52.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:52.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:52.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:52.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:52.528+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:52.531+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:52.531+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:52.534+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:52.536+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:52.537+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:52.537+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:52.537+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:52.541+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:52.543+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:52.546+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:52.547+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:52.549+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 22) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:52.553+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.554+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 35) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.555+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 6) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00005-8de414d7-5fe0-476f-80fa-8b132004bcf7-c000.avro
[2025-08-18T14:00:52.555+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:52.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:52.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:52.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:52.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:52.560+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:52.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)
[2025-08-18T14:00:52.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:52.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:52.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:52.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:52.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:52.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:52.576+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:52.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:52.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:52.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:52.580+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:52.585+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:52.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:52.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:52.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:52.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:52.591+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:52.594+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:52.594+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:52.595+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:52.598+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.598+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 1.2 in stage 1.0 (TID 29) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:52.601+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:52.699+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.712+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.3 in stage 1.0 (TID 36) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.759+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 1.2 in stage 3.0 (TID 33) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:52.760+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:52.761+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.12:44081 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:52.769+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.12:44081 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:52.775+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 37) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.793+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:52.801+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.12:44081 (size: 6.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:52.808+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 0.1 in stage 4.0 (TID 38) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:52.813+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 1.1 in stage 5.0 (TID 24) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:52.815+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.815+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 23) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:52.816+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:52.981+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.1 in stage 4.0 (TID 39) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:52.988+0000] {subprocess.py:93} INFO - 25/08/18 14:00:52 INFO TaskSetManager: Starting task 1.3 in stage 3.0 (TID 40) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:53.038+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Lost task 0.1 in stage 7.0 (TID 28) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00005-12fc1c68-c95d-4a9e-bb2e-2e537a52ff9f-c000.avro
[2025-08-18T14:00:53.039+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:53.040+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:53.060+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Lost task 1.1 in stage 7.0 (TID 27) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-12fc1c68-c95d-4a9e-bb2e-2e537a52ff9f-c000.avro
[2025-08-18T14:00:53.061+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:53.141+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID 41) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:53.166+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Lost task 1.3 in stage 1.0 (TID 36) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:53.167+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:53.204+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 ERROR TaskSetManager: Task 1 in stage 1.0 failed 4 times; aborting job
[2025-08-18T14:00:53.267+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 42) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:53.276+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Lost task 1.3 in stage 3.0 (TID 40) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:53.277+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:53.290+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 ERROR TaskSetManager: Task 1 in stage 3.0 failed 4 times; aborting job
[2025-08-18T14:00:53.369+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Starting task 1.2 in stage 5.0 (TID 43) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:53.375+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Lost task 1.1 in stage 6.0 (TID 26) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-11231ceb-99cd-4f85-8916-e84173e105b6-c000.avro
[2025-08-18T14:00:53.375+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:53.407+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.12:44081 (size: 34.7 KiB, free: 434.3 MiB)
[2025-08-18T14:00:53.416+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSchedulerImpl: Cancelling stage 1
[2025-08-18T14:00:53.492+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 36) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:53.494+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:53.495+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:53.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:53.498+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:53.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:53.501+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:53.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T14:00:53.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:53.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:53.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:53.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:53.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:53.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:53.515+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:53.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:53.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:53.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:53.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:53.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:53.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:53.525+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:53.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:53.528+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:53.530+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:53.533+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:53.534+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:53.536+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:53.537+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T14:00:53.583+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T14:00:53.602+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:53.603+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T14:00:53.725+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.12:44081 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:53.787+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:44081 (size: 34.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:53.831+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSchedulerImpl: Stage 1 was cancelled
[2025-08-18T14:00:53.880+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 343.512 s due to Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 36) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:53.884+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:53.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:53.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:53.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:53.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:53.885+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:53.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T14:00:53.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:53.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:53.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:53.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:53.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:53.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:53.914+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:53.914+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:53.915+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:53.915+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:53.915+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:53.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:53.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:53.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:53.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:53.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:53.930+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:53.939+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:53.943+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:53.944+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:53.944+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T14:00:53.945+0000] {subprocess.py:93} INFO - 25/08/18 14:00:53 INFO TaskSetManager: Starting task 1.2 in stage 6.0 (TID 44) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.008+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.2 in stage 7.0 (TID 45) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.009+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.2 in stage 7.0 (TID 46) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.040+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 47) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.043+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 41) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00005-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:54.044+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.078+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Job 0 failed: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 344.528831 s
[2025-08-18T14:00:54.096+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.111+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 48) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.133+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 49) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.141+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 8a8e78c5-6451-4ec7-b661-d83c355304bf)
[2025-08-18T14:00:54.163+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId bc7805c0-4d51-4a70-b0c4-36041a1ecb97)
[2025-08-18T14:00:54.165+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 20581655-f967-49f8-83b5-553970fce88b)
[2025-08-18T14:00:54.167+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId fed2bc33-f31e-4953-b728-b899bf3f5bb2)
[2025-08-18T14:00:54.172+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId ece0fb74-97e7-4d34-a6b7-861daab28170)
[2025-08-18T14:00:54.174+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.14:36279 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T14:00:54.175+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.13:42473 (size: 34.7 KiB, free: 434.1 MiB)
[2025-08-18T14:00:54.183+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.2 in stage 5.0 (TID 43) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:54.184+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.185+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 37) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00005-662e3d18-5c46-4180-8ba6-a16ede2f2a43-c000.avro
[2025-08-18T14:00:54.188+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:54.191+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 91ff8648-fd0a-4d26-bcd3-5c250ddf3580)
[2025-08-18T14:00:54.193+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId d68c0be1-b760-4487-93c6-fd1e1e7bd2b6)
[2025-08-18T14:00:54.197+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId ba416dfe-be65-4005-82dc-c49f28a72ee3)
[2025-08-18T14:00:54.200+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: Asked to cancel jobs with tag broadcast exchange (runId 07c85e06-cd56-4083-91af-cd92c5ed09c3)
[2025-08-18T14:00:54.202+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 34) (172.18.0.12 executor 2): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/FR_category_id/part-00005-291829a2-283a-4f37-a00c-c4d504d135c6-c000.avro
[2025-08-18T14:00:54.206+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:54.209+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:54.211+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:54.213+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:54.215+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:54.216+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:54.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
[2025-08-18T14:00:54.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:54.222+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:54.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:54.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:54.230+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:54.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:54.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:54.237+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:54.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:54.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:54.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:54.245+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:54.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:54.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:54.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:54.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:54.248+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:54.251+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:54.253+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:54.253+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:54.254+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.2 in stage 2.0 (TID 32) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:54.254+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.254+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 42) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00005-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:54.255+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.259+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.1 in stage 6.0 (TID 25) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00005-11231ceb-99cd-4f85-8916-e84173e105b6-c000.avro
[2025-08-18T14:00:54.262+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:54.263+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 50) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.265+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 51) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.275+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 52) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.280+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 3
[2025-08-18T14:00:54.282+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage cancelled: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 40) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:54.282+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:54.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:54.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:54.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:54.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:54.285+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:54.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-18T14:00:54.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:54.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:54.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:54.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:54.300+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:54.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:54.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:54.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:54.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:54.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:54.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:54.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:54.312+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:54.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:54.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:54.320+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:54.323+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:54.324+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:54.324+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:54.333+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:54.334+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T14:00:54.338+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 35) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00011-662e3d18-5c46-4180-8ba6-a16ede2f2a43-c000.avro
[2025-08-18T14:00:54.339+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:54.346+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 343.820 s due to Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 40) (172.18.0.13 executor 1): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/US_category_id/part-00003-ef2ee1bf-ff41-4142-9643-ee1b9850aa59-c000.avro
[2025-08-18T14:00:54.348+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:54.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:54.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:54.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:54.361+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:54.361+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:54.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
[2025-08-18T14:00:54.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:54.366+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:54.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:54.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:54.374+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:54.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:54.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:54.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:54.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:54.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:54.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:54.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:54.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:54.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:54.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:54.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:54.394+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:54.395+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:54.396+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:54.397+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:54.398+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T14:00:54.398+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.13:42473 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T14:00:54.398+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 53) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.399+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 30) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00005-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:54.399+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:54.399+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.399+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.2 in stage 2.0 (TID 31) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:54.406+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.407+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 54) (172.18.0.14, executor 0, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.408+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.3 in stage 5.0 (TID 55) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.410+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.2 in stage 6.0 (TID 44) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-11231ceb-99cd-4f85-8916-e84173e105b6-c000.avro
[2025-08-18T14:00:54.411+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.442+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.14:36279 (size: 6.7 KiB, free: 434.1 MiB)
[2025-08-18T14:00:54.443+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.2 in stage 7.0 (TID 46) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00005-12fc1c68-c95d-4a9e-bb2e-2e537a52ff9f-c000.avro
[2025-08-18T14:00:54.444+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.446+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.3 in stage 6.0 (TID 56) (172.18.0.13, executor 1, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.478+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.2 in stage 6.0 (TID 57) (172.18.0.14, executor 0, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.488+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.1 in stage 4.0 (TID 38) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00005-8de414d7-5fe0-476f-80fa-8b132004bcf7-c000.avro
[2025-08-18T14:00:54.489+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:54.490+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 7
[2025-08-18T14:00:54.493+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage cancelled: Job 7 cancelled part of cancelled job tag broadcast exchange (runId 8a8e78c5-6451-4ec7-b661-d83c355304bf)
[2025-08-18T14:00:54.498+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.499+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 7 was cancelled
[2025-08-18T14:00:54.504+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 343.826 s due to Job 7 cancelled part of cancelled job tag broadcast exchange (runId 8a8e78c5-6451-4ec7-b661-d83c355304bf)
[2025-08-18T14:00:54.518+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 2
[2025-08-18T14:00:54.523+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job 1 cancelled part of cancelled job tag broadcast exchange (runId bc7805c0-4d51-4a70-b0c4-36041a1ecb97)
[2025-08-18T14:00:54.526+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 2 was cancelled
[2025-08-18T14:00:54.531+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.2 in stage 7.0 (TID 45) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/CA_category_id/part-00011-12fc1c68-c95d-4a9e-bb2e-2e537a52ff9f-c000.avro
[2025-08-18T14:00:54.535+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.538+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.543+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 344.113 s due to Job 1 cancelled part of cancelled job tag broadcast exchange (runId bc7805c0-4d51-4a70-b0c4-36041a1ecb97)
[2025-08-18T14:00:54.564+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.2 in stage 4.0 (TID 58) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.566+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.1 in stage 8.0 (TID 59) (172.18.0.13, executor 1, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.576+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 8
[2025-08-18T14:00:54.580+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage cancelled: Job 8 cancelled part of cancelled job tag broadcast exchange (runId 20581655-f967-49f8-83b5-553970fce88b)
[2025-08-18T14:00:54.580+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 8 was cancelled
[2025-08-18T14:00:54.581+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 343.815 s due to Job 8 cancelled part of cancelled job tag broadcast exchange (runId 20581655-f967-49f8-83b5-553970fce88b)
[2025-08-18T14:00:54.581+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 51) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00005-662e3d18-5c46-4180-8ba6-a16ede2f2a43-c000.avro
[2025-08-18T14:00:54.589+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.591+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.1 in stage 4.0 (TID 39) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/MX_category_id/part-00011-8de414d7-5fe0-476f-80fa-8b132004bcf7-c000.avro
[2025-08-18T14:00:54.593+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 1]
[2025-08-18T14:00:54.595+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 60) (172.18.0.12, executor 2, partition 0, ANY, 9620 bytes)
[2025-08-18T14:00:54.606+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.12:44081 (size: 6.7 KiB, free: 434.2 MiB)
[2025-08-18T14:00:54.609+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 50) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00005-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:54.610+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:54.612+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 9
[2025-08-18T14:00:54.616+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId fed2bc33-f31e-4953-b728-b899bf3f5bb2)
[2025-08-18T14:00:54.620+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 9 was cancelled
[2025-08-18T14:00:54.621+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 343.816 s due to Job 9 cancelled part of cancelled job tag broadcast exchange (runId fed2bc33-f31e-4953-b728-b899bf3f5bb2)
[2025-08-18T14:00:54.621+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 6
[2025-08-18T14:00:54.622+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage cancelled: Job 6 cancelled part of cancelled job tag broadcast exchange (runId ece0fb74-97e7-4d34-a6b7-861daab28170)
[2025-08-18T14:00:54.628+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 6 was cancelled
[2025-08-18T14:00:54.647+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Starting task 1.2 in stage 4.0 (TID 61) (172.18.0.12, executor 2, partition 1, ANY, 9620 bytes)
[2025-08-18T14:00:54.654+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 344.006 s due to Job 6 cancelled part of cancelled job tag broadcast exchange (runId ece0fb74-97e7-4d34-a6b7-861daab28170)
[2025-08-18T14:00:54.655+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-08-18T14:00:54.656+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job 2 cancelled part of cancelled job tag broadcast exchange (runId 91ff8648-fd0a-4d26-bcd3-5c250ddf3580)
[2025-08-18T14:00:54.656+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 0 was cancelled
[2025-08-18T14:00:54.657+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 344.525 s due to Job 2 cancelled part of cancelled job tag broadcast exchange (runId 91ff8648-fd0a-4d26-bcd3-5c250ddf3580)
[2025-08-18T14:00:54.664+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 53) on 172.18.0.12, executor 2: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/KR_category_id/part-00011-662e3d18-5c46-4180-8ba6-a16ede2f2a43-c000.avro
[2025-08-18T14:00:54.666+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 2]
[2025-08-18T14:00:54.714+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 4
[2025-08-18T14:00:54.715+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage cancelled: Job 3 cancelled part of cancelled job tag broadcast exchange (runId d68c0be1-b760-4487-93c6-fd1e1e7bd2b6)
[2025-08-18T14:00:54.716+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 4 was cancelled
[2025-08-18T14:00:54.718+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 344.186 s due to Job 3 cancelled part of cancelled job tag broadcast exchange (runId d68c0be1-b760-4487-93c6-fd1e1e7bd2b6)
[2025-08-18T14:00:54.728+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.3 in stage 2.0 (TID 54) on 172.18.0.14, executor 0: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/DE_category_id/part-00011-f19a7bde-c19b-4608-8a5d-c9316efdab11-c000.avro
[2025-08-18T14:00:54.729+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:54.731+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.731+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Cancelling stage 5
[2025-08-18T14:00:54.732+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage cancelled: Job 5 cancelled part of cancelled job tag broadcast exchange (runId ba416dfe-be65-4005-82dc-c49f28a72ee3)
[2025-08-18T14:00:54.737+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Stage 5 was cancelled
[2025-08-18T14:00:54.739+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) failed in 344.142 s due to Job 5 cancelled part of cancelled job tag broadcast exchange (runId ba416dfe-be65-4005-82dc-c49f28a72ee3)
[2025-08-18T14:00:54.840+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 0.1 in stage 8.0 (TID 59) (172.18.0.13 executor 1): TaskKilled (Stage cancelled: Job 8 cancelled part of cancelled job tag broadcast exchange (runId 20581655-f967-49f8-83b5-553970fce88b))
[2025-08-18T14:00:54.853+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 48) (172.18.0.14 executor 0): TaskKilled (Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId fed2bc33-f31e-4953-b728-b899bf3f5bb2))
[2025-08-18T14:00:54.854+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.857+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 0.2 in stage 6.0 (TID 57) (172.18.0.14 executor 0): TaskKilled (Stage cancelled: Job 6 cancelled part of cancelled job tag broadcast exchange (runId ece0fb74-97e7-4d34-a6b7-861daab28170))
[2025-08-18T14:00:54.861+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.3 in stage 5.0 (TID 55) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/RU_category_id/part-00011-f0de17dc-9ba1-4bdb-96ed-8966c1b773e2-c000.avro
[2025-08-18T14:00:54.862+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:54.862+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 47) (172.18.0.13 executor 1): TaskKilled (Stage cancelled: Job 8 cancelled part of cancelled job tag broadcast exchange (runId 20581655-f967-49f8-83b5-553970fce88b))
[2025-08-18T14:00:54.862+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.866+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSetManager: Lost task 1.3 in stage 6.0 (TID 56) on 172.18.0.13, executor 1: org.apache.spark.SparkFileNotFoundException (File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/JP_category_id/part-00011-11231ceb-99cd-4f85-8916-e84173e105b6-c000.avro
[2025-08-18T14:00:54.869+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.) [duplicate 3]
[2025-08-18T14:00:54.872+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.873+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 49) (172.18.0.14 executor 0): TaskKilled (Stage cancelled: Job 9 cancelled part of cancelled job tag broadcast exchange (runId fed2bc33-f31e-4953-b728-b899bf3f5bb2))
[2025-08-18T14:00:54.873+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.914+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 1.2 in stage 4.0 (TID 61) (172.18.0.12 executor 2): TaskKilled (Stage cancelled: Job 3 cancelled part of cancelled job tag broadcast exchange (runId d68c0be1-b760-4487-93c6-fd1e1e7bd2b6))
[2025-08-18T14:00:54.918+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 0.3 in stage 0.0 (TID 60) (172.18.0.12 executor 2): TaskKilled (Stage cancelled: Job 2 cancelled part of cancelled job tag broadcast exchange (runId 91ff8648-fd0a-4d26-bcd3-5c250ddf3580))
[2025-08-18T14:00:54.919+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-08-18T14:00:54.919+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 WARN TaskSetManager: Lost task 0.2 in stage 4.0 (TID 58) (172.18.0.12 executor 2): TaskKilled (Stage cancelled: Job 3 cancelled part of cancelled job tag broadcast exchange (runId d68c0be1-b760-4487-93c6-fd1e1e7bd2b6))
[2025-08-18T14:00:54.920+0000] {subprocess.py:93} INFO - 25/08/18 14:00:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-08-18T14:00:55.006+0000] {subprocess.py:93} INFO - 25/08/18 14:00:55 WARN TaskSetManager: Lost task 0.3 in stage 5.0 (TID 52) (172.18.0.12 executor 2): TaskKilled (Stage cancelled: Job 5 cancelled part of cancelled job tag broadcast exchange (runId ba416dfe-be65-4005-82dc-c49f28a72ee3))
[2025-08-18T14:00:55.006+0000] {subprocess.py:93} INFO - 25/08/18 14:00:55 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-08-18T14:00:56.400+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-08-18T14:00:56.400+0000] {subprocess.py:93} INFO -   File "/opt/***/jobs/transformation/transform_to_golden_dataset.py", line 135, in <module>
[2025-08-18T14:00:56.401+0000] {subprocess.py:93} INFO -     final_df.write.format("parquet").mode("overwrite").save("hdfs://namenode:9000/storage/hdfs/processed/golden_dataset")
[2025-08-18T14:00:56.401+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1463, in save
[2025-08-18T14:00:56.401+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-08-18T14:00:56.402+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-08-18T14:00:56.402+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-08-18T14:00:56.420+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o1969.save.
[2025-08-18T14:00:56.421+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 36) (172.18.0.14 executor 0): org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:56.422+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:56.422+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:56.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:56.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:56.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:56.427+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:56.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T14:00:56.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:56.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:56.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:56.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:56.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:56.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:56.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:56.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:56.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:56.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:56.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:56.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:56.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:56.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:56.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:56.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:56.444+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:56.444+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:56.444+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:56.444+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:56.445+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-08-18T14:00:56.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-08-18T14:00:56.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-08-18T14:00:56.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-08-18T14:00:56.446+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-08-18T14:00:56.446+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-08-18T14:00:56.447+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-08-18T14:00:56.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-08-18T14:00:56.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-08-18T14:00:56.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-08-18T14:00:56.451+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-08-18T14:00:56.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-08-18T14:00:56.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-08-18T14:00:56.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-08-18T14:00:56.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-08-18T14:00:56.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-08-18T14:00:56.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-08-18T14:00:56.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-08-18T14:00:56.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-08-18T14:00:56.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-08-18T14:00:56.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
[2025-08-18T14:00:56.457+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
[2025-08-18T14:00:56.457+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2025-08-18T14:00:56.457+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2025-08-18T14:00:56.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
[2025-08-18T14:00:56.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
[2025-08-18T14:00:56.462+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)
[2025-08-18T14:00:56.462+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)
[2025-08-18T14:00:56.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)
[2025-08-18T14:00:56.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-08-18T14:00:56.465+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)
[2025-08-18T14:00:56.466+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2025-08-18T14:00:56.466+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-08-18T14:00:56.466+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-08-18T14:00:56.466+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-08-18T14:00:56.467+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkFileNotFoundException: File does not exist: hdfs://namenode:9000/storage/hdfs/raw/avro/GB_category_id/part-00011-ca07fc34-d358-49e8-9726-0d5169ef9826-c000.avro
[2025-08-18T14:00:56.467+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
[2025-08-18T14:00:56.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)
[2025-08-18T14:00:56.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)
[2025-08-18T14:00:56.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)
[2025-08-18T14:00:56.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-08-18T14:00:56.473+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-08-18T14:00:56.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
[2025-08-18T14:00:56.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-08-18T14:00:56.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-08-18T14:00:56.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-08-18T14:00:56.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-08-18T14:00:56.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-08-18T14:00:56.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-08-18T14:00:56.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-08-18T14:00:56.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-08-18T14:00:56.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-08-18T14:00:56.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-08-18T14:00:56.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-08-18T14:00:56.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-08-18T14:00:56.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-08-18T14:00:56.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-08-18T14:00:56.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-08-18T14:00:56.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-08-18T14:00:56.484+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-08-18T14:00:56.484+0000] {subprocess.py:93} INFO - 
[2025-08-18T14:00:56.653+0000] {subprocess.py:93} INFO - 25/08/18 14:00:56 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-18T14:00:56.659+0000] {subprocess.py:93} INFO - 25/08/18 14:00:56 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-08-18T14:00:56.726+0000] {subprocess.py:93} INFO - 25/08/18 14:00:56 INFO SparkUI: Stopped Spark web UI at http://4bf6b336ccb3:4043
[2025-08-18T14:00:56.737+0000] {subprocess.py:93} INFO - 25/08/18 14:00:56 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-18T14:00:56.741+0000] {subprocess.py:93} INFO - 25/08/18 14:00:56 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-18T14:00:57.243+0000] {subprocess.py:93} INFO - 25/08/18 14:00:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-18T14:00:57.840+0000] {subprocess.py:93} INFO - 25/08/18 14:00:57 INFO MemoryStore: MemoryStore cleared
[2025-08-18T14:00:57.841+0000] {subprocess.py:93} INFO - 25/08/18 14:00:57 INFO BlockManager: BlockManager stopped
[2025-08-18T14:00:58.006+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-18T14:00:58.046+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-18T14:00:58.281+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO SparkContext: Successfully stopped SparkContext
[2025-08-18T14:00:58.282+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO ShutdownHookManager: Shutdown hook called
[2025-08-18T14:00:58.298+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca094dea-b92a-4af1-9e2b-292ae7375e47/pyspark-ae1ba02c-25b5-49d5-a7ac-3ddbe8fe84b1
[2025-08-18T14:00:58.386+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-b667a502-006d-426e-b117-24514d5cd6a6
[2025-08-18T14:00:58.423+0000] {subprocess.py:93} INFO - 25/08/18 14:00:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca094dea-b92a-4af1-9e2b-292ae7375e47
[2025-08-18T14:00:58.954+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-08-18T14:00:58.956+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-08-18T14:00:58.990+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-08-18T14:00:58.997+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=youtube_pipeline_batch_data, task_id=transform_golden, execution_date=20250818T134521, start_date=20250818T135232, end_date=20250818T140058
[2025-08-18T14:00:59.032+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 358 for task transform_golden (Bash command failed. The command returned a non-zero exit code 1.; 5597)
[2025-08-18T14:00:59.107+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-08-18T14:00:59.650+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-18T14:00:59.654+0000] {local_task_job_runner.py:222} INFO - ::endgroup::

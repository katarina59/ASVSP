version: "3.8"

services:
  zookeeper:
    image: bitnami/zookeeper:3.9.2
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"

  kafka:
    image: bitnami/kafka:3.6.1-debian-11-r0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper


  spark-master:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=master
      - SPARK_WORKER_MEMORY=8G
      - SPARK_WORKER_CORES=4
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    volumes:
      - ./app:/app
      - ./batch:/app/batch
      - ./data/raw:/app/data/raw

  spark-worker:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    deploy:
      replicas: 3

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - HADOOP_HEAPSIZE=256
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env


  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env

  batch_ingestion:
    build: ./batch
    depends_on:
      - spark-master
      - namenode
    volumes:
      - ./data/raw:/app/data/raw
      - ./storage/hdfs/raw:/app/storage/hdfs/raw
    tty: true
    stdin_open: true


  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow:
    image: apache/airflow:2.9.0-python3.9
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: my_secret_key
    depends_on:
      - postgres
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: >
      bash -c "airflow db migrate &&
               airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com &&
               airflow webserver"

# U airflow servisu zameni airflow db migrate sa airflow db upgrade, jer migrate više nije preporučen:
# # command: >
#   bash -c "airflow db upgrade &&
#            airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com &&
#            airflow webserver"


  cloudera:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8888:8888"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./conf.dist:/usr/share/hue/desktop/conf
    depends_on: 
      - namenode
      
  superset:
    image: apache/superset:3.0.0
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=some_very_secret_key
    depends_on:
      - postgres


volumes:
  hdfs_namenode:
  hdfs_datanode:
  postgres_data:

version: "3.8"

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.9.3-python3.11
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: ""
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./batch:/opt/airflow/jobs
    - ./sql:/opt/airflow/sql
    - /var/run/docker.sock:/var/run/docker.sock
  user: "${AIRFLOW_UID}:0"
  depends_on:
      postgres:
        condition: service_healthy
  networks:
      - kafka-network

services:
  # airflow:
  #   image: apache/airflow:2.9.3-python3.11
  #   container_name: airflow
  #   restart: always
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #     - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
  #     - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  #     - AIRFLOW_UID=${AIRFLOW_UID}
  #     - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC
  #     - AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE=UTC
  #     - TZ=UTC
  #   volumes:
  #     - ./airflow/dags:/opt/airflow/dags
  #     - ./airflow/logs:/opt/airflow/logs
  #     - ./airflow/plugins:/opt/airflow/plugins
  #     - ./batch:/opt/airflow/jobs
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   ports:
  #     - "${AIRFLOW_PORT}:8080"
  #   command: >
  #     bash -c "
  #     echo 'Postgres is healthy, initializing Airflow database...' &&
  #     airflow db init &&
  #     echo 'Database initialized successfully!' &&
  #     airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || echo 'User already exists' &&
  #     echo 'Starting Airflow services...' &&
  #     airflow webserver --port 8080 &
  #     airflow scheduler
  #     "
  #   networks:
  #     - kafka-network

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "${AIRFLOW_PORT}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    command: bash -c "airflow db migrate && airflow users create \
      --username admin \
      --password admin \
      --firstname Admin \
      --lastname User \
      --role Admin \
      --email admin@example.com"


  zookeeper:
    image: bitnami/zookeeper:3.9.2
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2182:2181"
    networks:
      - kafka-network

  kafka:
    image: bitnami/kafka:3.6.1-debian-11-r0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5

  producer:
    build: ./streaming/kafka_producer
    container_name: producer
    environment:
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - kafka-network

  # consumer:
  #   build: ./streaming/kafka_consumer
  #   container_name: consumer
  #   environment:
  #     - KAFKA_BROKERS=kafka:9092
  #     - KAFKA_TOPIC=youtube_realtime
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   restart: unless-stopped
  #   networks:
  #     - kafka-network

  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_WORKER_MEMORY=8G
      - SPARK_WORKER_CORES=4
      #dodato za real-time
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    volumes:
      - ./app:/app
      - ./batch:/opt/airflow/jobs
      - ./data/raw:/app/data/raw
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
    networks:
      - kafka-network

  # spark-worker:
  #   image: bitnami/spark:3.5.1
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     #dodato za real-time
  #     - SPARK_WORKER_MEMORY=2g
  #     - SPARK_EXECUTOR_MEMORY=1g
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   depends_on:
  #     - spark-master 
  #   deploy:
  #     replicas: 3
  #   volumes:
  #     - ./streaming/spark_streaming:/opt/spark_apps
  #     - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
  #   networks:
  #     - kafka-network

  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master 
    volumes:
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
    networks:
      - kafka-network

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master 
    volumes:
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
    networks:
      - kafka-network

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - HADOOP_HEAPSIZE=256
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
      #- ./data:/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env
    networks:
      - kafka-network


  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env
    networks:
      - kafka-network

  batch_ingestion:
    build: ./batch
    container_name: batch_ingestion
    depends_on:
      - spark-master
      - namenode
    volumes:
      - ./data/raw:/app/data/raw
      - ./storage/hdfs/raw:/app/storage/hdfs/raw
    tty: true
    stdin_open: true
    networks:
      - kafka-network


  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: ${POSTGRES_DB}    
      MB_DB_PORT: 5432
      MB_DB_USER: ${POSTGRES_USER}
      MB_DB_PASS: ${POSTGRES_PASSWORD}
      MB_DB_HOST: postgres
    restart: unless-stopped       
    depends_on:
      - postgres
    networks:
    - kafka-network
       


  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_TZ: "UTC"
      PGTZ: "UTC"
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  cloudera:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8085:8888"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./conf.dist:/usr/share/hue/desktop/conf
    depends_on: 
      - namenode
    networks:
      - kafka-network


volumes:
  hdfs_namenode:
  hdfs_datanode:
  postgres-db-volume:
  spark_logs:
  spark_apps_data:

networks:
  kafka-network:
    driver: bridge
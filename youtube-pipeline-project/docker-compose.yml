version: "3.8"

# x-airflow-common:
#   &airflow-common
#   image: apache/airflow:2.8.1
#   environment:
#     &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: SequentialExecutor
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
#     # _PIP_ADDITIONAL_REQUIREMENTS: "Flask-Session==0.4.0"
#     # AIRFLOW__DATABASE__SQL_ALCHEMY_SCHEMA: public
#     AIRFLOW__WEBSERVER__SESSION_USE_SIGNER: "false" 
#     # AIRFLOW__WEBSERVER__SESSION_BACKEND: database
#     AIRFLOW__WEBSERVER__SECRET_KEY: mysecretkey123
#     AIRFLOW__CORE__DEFAULT_TIMEZONE: UTC
#     # AIRFLOW__WEBSERVER__SESSION_TYPE: filesystem
#   volumes:
#     - ./airflow/dags:/opt/airflow/dags
#     - ./airflow/logs:/opt/airflow/logs
#     - ./airflow/plugins:/opt/airflow/plugins
#     - ./batch:/opt/airflow/jobs
#     - /var/run/docker.sock:/var/run/docker.sock
#     - ./airflow/db:/opt/airflow
#   user: "${AIRFLOW_UID}:0"
#   networks:
#     - kafka-network

services:
  
  airflow:
    image: apache/airflow:2.9.3
    container_name: airflow
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC
      - AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE=UTC
      - TZ=UTC
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./batch:/opt/airflow/jobs
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "${AIRFLOW_PORT}:8080"
    command: >
      bash -c "
      echo 'Postgres is healthy, initializing Airflow database...' &&
      airflow db init &&
      echo 'Database initialized successfully!' &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || echo 'User already exists' &&
      echo 'Starting Airflow services...' &&
      airflow webserver --port 8080 &
      airflow scheduler
      "
    networks:
      - kafka-network


  zookeeper:
    image: bitnami/zookeeper:3.9.2
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2182:2181"
    networks:
      - kafka-network

  kafka:
    image: bitnami/kafka:3.6.1-debian-11-r0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5

  producer:
    build: ./streaming/kafka_producer
    container_name: producer
    environment:
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - kafka-network

  # consumer:
  #   build: ./streaming/kafka_consumer
  #   container_name: consumer
  #   environment:
  #     - KAFKA_BROKERS=kafka:9092
  #     - KAFKA_TOPIC=youtube_realtime
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   restart: unless-stopped
  #   networks:
  #     - kafka-network

  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_WORKER_MEMORY=8G
      - SPARK_WORKER_CORES=4
      #dodato za real-time
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    volumes:
      - ./app:/app
      - ./batch:/opt/airflow/jobs
      - ./data/raw:/app/data/raw
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
    networks:
      - kafka-network

  # spark-worker:
  #   image: bitnami/spark:3.5.1
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     #dodato za real-time
  #     - SPARK_WORKER_MEMORY=2g
  #     - SPARK_EXECUTOR_MEMORY=1g
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   depends_on:
  #     - spark-master 
  #   deploy:
  #     replicas: 3
  #   volumes:
  #     - ./streaming/spark_streaming:/opt/spark_apps
  #     - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
  #   networks:
  #     - kafka-network

  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master 
    volumes:
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
    networks:
      - kafka-network

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master 
    volumes:
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar
    networks:
      - kafka-network

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - HADOOP_HEAPSIZE=256
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
      #- ./data:/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env
    networks:
      - kafka-network


  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env
    networks:
      - kafka-network

  batch_ingestion:
    build: ./batch
    container_name: batch_ingestion
    depends_on:
      - spark-master
      - namenode
    volumes:
      - ./data/raw:/app/data/raw
      - ./storage/hdfs/raw:/app/storage/hdfs/raw
    tty: true
    stdin_open: true
    networks:
      - kafka-network


  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: ${POSTGRES_DB}    
      MB_DB_PORT: 5432
      MB_DB_USER: ${POSTGRES_USER}
      MB_DB_PASS: ${POSTGRES_PASSWORD}
      MB_DB_HOST: postgres
    restart: unless-stopped       
    depends_on:
      - postgres
    networks:
    - kafka-network
       


  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_TZ: "UTC"
      PGTZ: "UTC"
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # airflow-webserver:
  #   <<: *airflow-common
  #   container_name: airflow_webserver
  #   command: webserver
  #   ports:
  #     - "${AIRFLOW_PORT}:8080"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #   restart: always
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-scheduler:
  #   <<: *airflow-common
  #   container_name: airflow_scheduler
  #   command: scheduler
  #   healthcheck:
  #     test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #   restart: always
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-init:
  #   <<: *airflow-common
  #   container_name: airflow_init
  #   command: bash -c "airflow db migrate && airflow users create \
  #     --username admin \
  #     --password admin \
  #     --firstname Admin \
  #     --lastname User \
  #     --role Admin \
  #     --email admin@example.com"
  #   restart: "no" 


  cloudera:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8085:8888"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./conf.dist:/usr/share/hue/desktop/conf
    depends_on: 
      - namenode
    networks:
      - kafka-network


volumes:
  hdfs_namenode:
  hdfs_datanode:
  postgres-db-volume:
  spark_logs:
  spark_apps_data:

networks:
  kafka-network:
    driver: bridge
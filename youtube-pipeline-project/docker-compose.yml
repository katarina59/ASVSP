version: "3.8"

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.1
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: ""
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./batch:/opt/airflow/jobs
    - ./sql:/opt/airflow/sql
    - /var/run/docker.sock:/var/run/docker.sock
  user: "${AIRFLOW_UID}:0"
  depends_on:
    - postgres

services:
  # zookeeper:
  #   image: bitnami/zookeeper:3.9.2
  #   container_name: zookeeper
  #   environment:
  #     - ALLOW_ANONYMOUS_LOGIN=yes
  #   ports:
  #     - "2181:2181"

  # kafka:
  #   image: bitnami/kafka:3.6.1-debian-11-r0
  #   container_name: kafka
  #   ports:
  #     - "9092:9092"
  #   environment:
  #     - KAFKA_BROKER_ID=1
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #   depends_on:
  #     - zookeeper

  # producer:
  #   build: ./streaming/kafka_producer
  #   container_name: producer
  #   depends_on:
  #     - kafka
  #   restart: unless-stopped

  # consumer:
  #   build: ./streaming/kafka_consumer
  #   container_name: consumer
  #   depends_on:
  #     - kafka
  #   restart: unless-stopped

  # zk-druid:
  #   image: zookeeper:3.8
  #   container_name: zk-druid
  #   environment:
  #     ZOO_MY_ID: 1
  #     ZOO_SERVERS: server.1=zk-druid:2888:3888;2181
  #   ports:
  #     - "2182:2181"

  # # DRUID COORDINATOR
  # druid-coordinator:
  #   image: apache/druid:28.0.1
  #   container_name: druid-coordinator
  #   user: "0:0"
  #   environment:
  #     - DRUID_XMX=1g
  #     - DRUID_XMS=1g
  #   volumes:
  #     - druid_shared:/opt/shared
  #     - ./druid-config:/opt/druid/conf/druid/single-server/nano-quickstart
  #   ports:
  #     - "8084:8081"
  #   command:
  #     - coordinator
  #   depends_on:
  #     - zk-druid

  # druid-broker:
  #   image: apache/druid:28.0.1
  #   container_name: druid-broker
  #   user: "0:0"
  #   environment:
  #     - DRUID_XMX=1g
  #     - DRUID_XMS=1g
  #   volumes:
  #     - druid_shared:/opt/shared
  #     - ./druid-config:/opt/druid/conf/druid/single-server/nano-quickstart
  #   ports:
  #     - "8082:8082"
  #   command:
  #     - broker
  #   depends_on:
  #     - zk-druid

  # druid-historical:
  #   image: apache/druid:28.0.1
  #   container_name: druid-historical
  #   user: "0:0" 
  #   environment:
  #     - DRUID_XMX=1g
  #     - DRUID_XMS=1g
  #   volumes:
  #     - druid_shared:/opt/shared
  #     - druid_segments:/opt/druid/var/druid/segments
  #     - druid_indexing_logs:/opt/druid/var/druid/indexing-logs
  #     - druid_task:/opt/druid/var/druid/task
  #     - druid_hadoop_tmp:/opt/druid/var/druid/hadoop-tmp
  #     - druid_segment_cache:/opt/druid/var/druid/segment-cache
  #     - ./druid-config:/opt/druid/conf/druid/single-server/nano-quickstart
  #   ports:
  #     - "8083:8083"
  #   command:
  #     - historical
  #   depends_on:
  #     - zk-druid

  # druid-middlemanager:
  #   image: apache/druid:28.0.1
  #   container_name: druid-middlemanager
  #   user: "0:0" 
  #   environment:
  #     - DRUID_XMX=1g
  #     - DRUID_XMS=1g
  #   volumes:
  #     - druid_shared:/opt/shared
  #     - druid_task_logs:/opt/druid/var/druid/task
  #     - druid_segments:/opt/druid/var/druid/segments
  #     - druid_indexing_logs:/opt/druid/var/druid/indexing-logs
  #     - druid_hadoop_tmp:/opt/druid/var/druid/hadoop-tmp
  #     - druid_segment_cache:/opt/druid/var/druid/segment-cache
  #     - ./druid-config:/opt/druid/conf/druid/single-server/nano-quickstart
  #   ports:
  #     - "8091:8091"
  #   command:
  #     - middleManager
  #   depends_on:
  #     - zk-druid

  # druid-router:
  #   image: apache/druid:28.0.1
  #   container_name: druid-router
  #   user: "0:0" 
  #   environment:
  #     - DRUID_XMX=512m
  #     - DRUID_XMS=512m
  #   volumes:
  #     - ./druid-config:/opt/druid/conf/druid/single-server/nano-quickstart
  #   ports:
  #     - "8085:8888"  # Druid Web Console
  #   command:
  #     - router
  #   depends_on:
  #     - druid-coordinator
  #     - druid-broker

  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_WORKER_MEMORY=8G
      - SPARK_WORKER_CORES=4
      #dodato za real-time
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    volumes:
      - ./app:/app
      - ./batch:/opt/airflow/jobs
      - ./data/raw:/app/data/raw
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar

  spark-worker:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      #dodato za real-time
      - SPARK_WORKER_MEMORY=2g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master 
    deploy:
      replicas: 3
    volumes:
      - ./streaming/spark_streaming:/opt/spark_apps
      - ./jars:/opt/spark/jars/postgresql-42.7.6.jar


  # spark-streaming:
  #   build: ./streaming/spark_streaming
  #   container_name: spark-streaming-app
  #   environment:
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - KAFKA_BROKERS=kafka:9092
  #     - DRUID_BROKER_HOST=druid-broker
  #     - DRUID_BROKER_PORT=8082
  #   depends_on:
  #     - kafka
  #     - druid-router
  #     - spark-master
  #   volumes:
  #     - ./streaming/spark_streaming:/opt/spark_apps
  #   restart: unless-stopped

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - HADOOP_HEAPSIZE=256
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
      #- ./data:/data
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env


  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    volumes:
      - hdfs_datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    env_file:
      - ./hdfs.env

  batch_ingestion:
    build: ./batch
    container_name: batch_ingestion
    depends_on:
      - spark-master
      - namenode
    volumes:
      - ./data/raw:/app/data/raw
      - ./storage/hdfs/raw:/app/storage/hdfs/raw
    tty: true
    stdin_open: true




  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: ${POSTGRES_DB}    # koristiš isti DB kao Postgres service
      MB_DB_PORT: 5432
      MB_DB_USER: ${POSTGRES_USER}
      MB_DB_PASS: ${POSTGRES_PASSWORD}
      MB_DB_HOST: postgres
    restart: unless-stopped        # ime service-a iz docker-compose
    depends_on:
      - postgres
       


  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "${AIRFLOW_PORT}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler

  airflow-init:
    <<: *airflow-common
    command: bash -c "airflow db migrate && airflow users create \
      --username admin \
      --password admin \
      --firstname Admin \
      --lastname User \
      --role Admin \
      --email admin@example.com"


  cloudera:
    image: gethue/hue:20201111-135001
    hostname: hue
    container_name: hue
    dns: 8.8.8.8
    ports:
      - "8888:8888"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./conf.dist:/usr/share/hue/desktop/conf
    depends_on: 
      - namenode
      


  # citus_coordinator:
  #   image: citusdata/citus:11.3
  #   container_name: citus_coordinator
  #   environment:
  #     POSTGRES_USER: citus
  #     POSTGRES_PASSWORD: citus
  #     POSTGRES_DB: youtube_dw
  #   ports:
  #     - "5433:5432"  # Da ne sudari sa postojećim Postgresom na 5432
  #   volumes:
  #     - citus_data:/var/lib/postgresql/data
  #   command: postgres -c shared_preload_libraries=citus

  # citus_worker1:
  #   image: citusdata/citus:11.3
  #   container_name: citus_worker1
  #   environment:
  #     POSTGRES_USER: citus
  #     POSTGRES_PASSWORD: citus
  #     POSTGRES_DB: youtube_dw
  #   volumes:
  #     - citus_worker1_data:/var/lib/postgresql/data
  #   command: postgres -c shared_preload_libraries=citus

  # citus_worker2:
  #   image: citusdata/citus:11.3
  #   container_name: citus_worker2
  #   environment:
  #     POSTGRES_USER: citus
  #     POSTGRES_PASSWORD: citus
  #     POSTGRES_DB: youtube_dw
  #   volumes:
  #     - citus_worker2_data:/var/lib/postgresql/data
  #   command: postgres -c shared_preload_libraries=citus


volumes:
  hdfs_namenode:
  hdfs_datanode:
  postgres-db-volume:
  # citus_data:
  # citus_worker1_data:
  # citus_worker2_data:
  druid_shared:
  druid_segments:
  druid_task_logs:
  druid_indexing_logs:
  druid_task:
  druid_hadoop_tmp:
  druid_segment_cache: